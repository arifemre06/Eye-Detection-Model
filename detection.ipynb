{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUUDHRLaAivR",
        "outputId": "ef5cb324-7e10-4ff0-d584-7e9cf8d5df5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow_gpu-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 4.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.25.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (21.3)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 65.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (4.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.46.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires keras<2.9,>=2.8.0rc0, but you have keras 2.9.0 which is incompatible.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.0 tensorflow-estimator-2.9.0 tensorflow-gpu-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-gpu "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOwmNWtLOV3E",
        "outputId": "b88854b6-39e2-4a16-c0cb-d12b1d8d053a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7ZaNYgABgPF",
        "outputId": "15c7cee2-db5f-4a7e-ebb9-a926335d37ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDUMtwbPBlXm",
        "outputId": "4e8d63be-822e-40e4-db60-8d489678da67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 73045, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 73045 (delta 35), reused 54 (delta 20), pack-reused 72957\u001b[K\n",
            "Receiving objects: 100% (73045/73045), 579.32 MiB | 28.67 MiB/s, done.\n",
            "Resolving deltas: 100% (51710/51710), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "z_0QbpW4B3wT",
        "outputId": "f12f4af7-9955-4b0f-956f-d0470055612a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBl0DgC2Di1x",
        "outputId": "e5eceea7-1e74-4a8c-f537-c10b673b4e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n"
          ]
        }
      ],
      "source": [
        "cd /content/models/research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5rj7LL5sDStC"
      },
      "outputs": [],
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNYw9fBCDSu4",
        "outputId": "68419a76-10f3-4fd2-8d68-68c819db42b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 975, done.\u001b[K\n",
            "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
            "Receiving objects: 100% (975/975), 11.72 MiB | 31.59 MiB/s, done.\n",
            "Resolving deltas: 100% (576/576), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7eH5XBDSw4",
        "outputId": "7114d83a-ff12-42ff-d26f-109927a176fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/cocoapi/PythonAPI\n"
          ]
        }
      ],
      "source": [
        "cd cocoapi/PythonAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKQjZrEvDSzB",
        "outputId": "6bb049a0-3d94-4b46-dd84-03830165af21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python setup.py build_ext --inplace\n",
            "running build_ext\n",
            "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/models/research/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> pycocotools\n",
            "rm -rf build\n"
          ]
        }
      ],
      "source": [
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O_3KyX09DS06"
      },
      "outputs": [],
      "source": [
        "cp -r pycocotools /content/models/research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SaBBGOZED2YY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "orS1i7W0D2an"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoR0wEdRDS3A",
        "outputId": "67d9a1aa-6975-42e2-e359-0a0f427875a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/cocoapi\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJxkupe7DyMq",
        "outputId": "f2b3a8cd-1693-4dc5-dcfa-1498d4debcd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n"
          ]
        }
      ],
      "source": [
        "cd .. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dPSMMMNJDyPH"
      },
      "outputs": [],
      "source": [
        "cp object_detection/packages/tf2/setup.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzP14LY1DyRL",
        "outputId": "178f0fea-9048-4e36-afc3-52abb77e1596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/models/research\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.38.0-cp37-cp37m-manylinux2010_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.30)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.9.2-py2.py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 57.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.9.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 69.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow~=2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 4.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Collecting tensorflow-text~=2.9.0\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 69.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.2 MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 74 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.31.5)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.56.1)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2022.1)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.10.8)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.46.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.25.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (4.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.8.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: pyarrow<7.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n",
            "Collecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.20.4-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.4.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 52.1 MB/s \n",
            "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 70.4 MB/s \n",
            "\u001b[?25hCollecting cloudpickle<3,>=2.0.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.8-cp37-cp37m-manylinux_2_24_x86_64.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Collecting protobuf>=3.12.0\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 63.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.12)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.16.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.7.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.4.0)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 44.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: object-detection, py-cpuinfo, dill, avro-python3, seqeval\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1692480 sha256=6c0b5c2ebc25a28a2d3140a9a18ec6598ce25e1fe27810ad5d6df6f70ac23aa4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z5isy15f/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=cf1d0cb292a41ebea8ec5307103c379f09f39d4a3b658d452dbe38d1e73fc6e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=3ca420cbaf764f8d2ad640989688c350442420b8729c83650766ff337963275c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=bd0ef14abe6fc1a838e09f6cd1056e33e3c5a2af4d75543e94248c2e79bcb2ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=c24f8bf411f758ee07d6c8aeef5fb6db923d64c5aac9d738064f348a62128b34\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built object-detection py-cpuinfo dill avro-python3 seqeval\n",
            "Installing collected packages: requests, protobuf, tensorflow-io-gcs-filesystem, tensorflow, portalocker, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, proto-plus, orjson, opencv-python-headless, hdfs, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.25.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.25.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.25.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0+zzzcolab20220506162203\n",
            "    Uninstalling tensorflow-2.8.0+zzzcolab20220506162203:\n",
            "      Successfully uninstalled tensorflow-2.8.0+zzzcolab20220506162203\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.1.1\n",
            "    Uninstalling pymongo-4.1.1:\n",
            "      Successfully uninstalled pymongo-4.1.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.38.0 avro-python3-1.10.2 cloudpickle-2.1.0 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.12 hdfs-2.7.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.5.64 orjson-3.6.8 portalocker-2.4.0 proto-plus-1.20.4 protobuf-3.20.1 py-cpuinfo-8.0.0 pymongo-3.12.3 pyyaml-5.4.1 requests-2.27.1 sacrebleu-2.1.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-2.9.0 tensorflow-addons-0.16.1 tensorflow-io-0.26.0 tensorflow-io-gcs-filesystem-0.26.0 tensorflow-model-optimization-0.7.2 tensorflow-text-2.9.0 tf-models-official-2.9.2 tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qztzSOiJ-GgV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkEtZu0ZDyTA",
        "outputId": "8e73506d-04f6-4b7b-e8e7-e198d6f60866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests under Python 3.7.13: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "2022-05-21 07:50:30.777291: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "W0521 07:50:31.139007 140049469417344 model_builder.py:1102] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.49s\n",
            "I0521 07:50:31.397968 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.49s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.51s\n",
            "I0521 07:50:31.905126 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.51s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.26s\n",
            "I0521 07:50:32.164004 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.26s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.24s\n",
            "I0521 07:50:32.401885 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.24s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.82s\n",
            "I0521 07:50:34.217864 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.82s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0521 07:50:34.218962 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
            "I0521 07:50:34.243951 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "I0521 07:50:34.257944 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "I0521 07:50:34.272007 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.09s\n",
            "I0521 07:50:34.365753 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.09s\n",
            "I0521 07:50:34.452266 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.1s\n",
            "I0521 07:50:34.550466 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.1s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.09s\n",
            "I0521 07:50:34.642526 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.09s\n",
            "I0521 07:50:34.734099 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n",
            "I0521 07:50:34.763363 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0521 07:50:34.935960 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0521 07:50:34.936159 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 64\n",
            "I0521 07:50:34.936250 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 3\n",
            "I0521 07:50:34.938884 140049469417344 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0521 07:50:34.957960 140049469417344 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0521 07:50:34.958130 140049469417344 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0521 07:50:35.026165 140049469417344 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0521 07:50:35.026462 140049469417344 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0521 07:50:35.195117 140049469417344 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0521 07:50:35.195299 140049469417344 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0521 07:50:35.348044 140049469417344 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0521 07:50:35.348199 140049469417344 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0521 07:50:35.570917 140049469417344 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0521 07:50:35.571072 140049469417344 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0521 07:50:35.945759 140049469417344 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0521 07:50:35.945938 140049469417344 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0521 07:50:36.258090 140049469417344 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0521 07:50:36.258282 140049469417344 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0521 07:50:36.333284 140049469417344 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0521 07:50:36.363648 140049469417344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0521 07:50:36.412371 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I0521 07:50:36.412522 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 88\n",
            "I0521 07:50:36.412609 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 4\n",
            "I0521 07:50:36.414324 140049469417344 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0521 07:50:36.432516 140049469417344 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0521 07:50:36.432675 140049469417344 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0521 07:50:36.560101 140049469417344 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0521 07:50:36.560242 140049469417344 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0521 07:50:36.786082 140049469417344 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0521 07:50:36.786242 140049469417344 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0521 07:50:37.018414 140049469417344 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0521 07:50:37.018610 140049469417344 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0521 07:50:37.323404 140049469417344 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0521 07:50:37.323570 140049469417344 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0521 07:50:37.627799 140049469417344 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0521 07:50:37.627969 140049469417344 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0521 07:50:38.014522 140049469417344 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0521 07:50:38.014686 140049469417344 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0521 07:50:38.163612 140049469417344 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0521 07:50:38.194401 140049469417344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0521 07:50:38.258742 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I0521 07:50:38.258878 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 112\n",
            "I0521 07:50:38.258943 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 5\n",
            "I0521 07:50:38.260435 140049469417344 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0521 07:50:38.275085 140049469417344 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0521 07:50:38.275193 140049469417344 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0521 07:50:38.396481 140049469417344 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0521 07:50:38.396606 140049469417344 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0521 07:50:38.628070 140049469417344 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0521 07:50:38.628229 140049469417344 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0521 07:50:38.853538 140049469417344 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0521 07:50:38.853715 140049469417344 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0521 07:50:39.156197 140049469417344 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0521 07:50:39.156393 140049469417344 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0521 07:50:39.454655 140049469417344 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0521 07:50:39.454811 140049469417344 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0521 07:50:39.831351 140049469417344 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0521 07:50:39.831528 140049469417344 efficientnet_model.py:143] round_filter input=320 output=352\n",
            "I0521 07:50:39.979221 140049469417344 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
            "I0521 07:50:40.007352 140049469417344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0521 07:50:40.064592 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I0521 07:50:40.064722 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 160\n",
            "I0521 07:50:40.064793 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 6\n",
            "I0521 07:50:40.066327 140049469417344 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0521 07:50:40.082221 140049469417344 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0521 07:50:40.082336 140049469417344 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0521 07:50:40.207659 140049469417344 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0521 07:50:40.207795 140049469417344 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0521 07:50:40.613642 140049469417344 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0521 07:50:40.613811 140049469417344 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0521 07:50:40.846565 140049469417344 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0521 07:50:40.846725 140049469417344 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0521 07:50:41.232273 140049469417344 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0521 07:50:41.232452 140049469417344 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0521 07:50:41.613433 140049469417344 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0521 07:50:41.613600 140049469417344 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0521 07:50:42.076058 140049469417344 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0521 07:50:42.076233 140049469417344 efficientnet_model.py:143] round_filter input=320 output=384\n",
            "I0521 07:50:42.221176 140049469417344 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
            "I0521 07:50:42.249678 140049469417344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0521 07:50:42.312047 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0521 07:50:42.312197 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 224\n",
            "I0521 07:50:42.312282 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 7\n",
            "I0521 07:50:42.314497 140049469417344 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0521 07:50:42.334912 140049469417344 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0521 07:50:42.335038 140049469417344 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0521 07:50:42.455920 140049469417344 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0521 07:50:42.456065 140049469417344 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0521 07:50:42.755215 140049469417344 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0521 07:50:42.755407 140049469417344 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0521 07:50:43.059190 140049469417344 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0521 07:50:43.059358 140049469417344 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0521 07:50:43.508118 140049469417344 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0521 07:50:43.508307 140049469417344 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0521 07:50:43.961461 140049469417344 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0521 07:50:43.961645 140049469417344 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0521 07:50:44.567052 140049469417344 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0521 07:50:44.567221 140049469417344 efficientnet_model.py:143] round_filter input=320 output=448\n",
            "I0521 07:50:44.712663 140049469417344 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
            "I0521 07:50:44.744075 140049469417344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0521 07:50:44.816946 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I0521 07:50:44.817082 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 288\n",
            "I0521 07:50:44.817149 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 7\n",
            "I0521 07:50:44.818735 140049469417344 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0521 07:50:44.833208 140049469417344 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0521 07:50:44.833327 140049469417344 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0521 07:50:45.014672 140049469417344 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0521 07:50:45.014823 140049469417344 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0521 07:50:45.392787 140049469417344 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0521 07:50:45.392991 140049469417344 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0521 07:50:45.954880 140049469417344 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0521 07:50:45.955057 140049469417344 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0521 07:50:46.487652 140049469417344 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0521 07:50:46.487826 140049469417344 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0521 07:50:47.017910 140049469417344 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0521 07:50:47.018096 140049469417344 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0521 07:50:47.699011 140049469417344 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0521 07:50:47.699212 140049469417344 efficientnet_model.py:143] round_filter input=320 output=512\n",
            "I0521 07:50:47.987525 140049469417344 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
            "I0521 07:50:48.033128 140049469417344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0521 07:50:48.150174 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0521 07:50:48.150344 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 384\n",
            "I0521 07:50:48.150443 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 8\n",
            "I0521 07:50:48.152077 140049469417344 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0521 07:50:48.167406 140049469417344 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0521 07:50:48.167547 140049469417344 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0521 07:50:48.356168 140049469417344 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0521 07:50:48.356334 140049469417344 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0521 07:50:48.814159 140049469417344 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0521 07:50:48.814344 140049469417344 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0521 07:50:49.275068 140049469417344 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0521 07:50:49.275236 140049469417344 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0521 07:50:49.909519 140049469417344 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0521 07:50:49.909696 140049469417344 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0521 07:50:50.593477 140049469417344 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0521 07:50:50.593647 140049469417344 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0521 07:50:51.712921 140049469417344 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0521 07:50:51.713124 140049469417344 efficientnet_model.py:143] round_filter input=320 output=576\n",
            "I0521 07:50:52.387955 140049469417344 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
            "I0521 07:50:52.437088 140049469417344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0521 07:50:52.641206 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I0521 07:50:52.641442 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 384\n",
            "I0521 07:50:52.641539 140049469417344 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 8\n",
            "I0521 07:50:52.644146 140049469417344 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0521 07:50:52.669182 140049469417344 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0521 07:50:52.669333 140049469417344 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0521 07:50:53.203696 140049469417344 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0521 07:50:53.203937 140049469417344 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0521 07:50:54.163874 140049469417344 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0521 07:50:54.164108 140049469417344 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0521 07:50:55.123092 140049469417344 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0521 07:50:55.123330 140049469417344 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0521 07:50:56.611878 140049469417344 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0521 07:50:56.612108 140049469417344 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0521 07:50:58.078344 140049469417344 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0521 07:50:58.078561 140049469417344 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0521 07:50:59.945636 140049469417344 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0521 07:50:59.945871 140049469417344 efficientnet_model.py:143] round_filter input=320 output=640\n",
            "I0521 07:51:00.487918 140049469417344 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
            "I0521 07:51:00.562019 140049469417344 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 26.01s\n",
            "I0521 07:51:00.772520 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 26.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I0521 07:51:00.780816 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0521 07:51:00.783612 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0521 07:51:00.784246 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0521 07:51:00.786611 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0521 07:51:00.788480 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0521 07:51:00.789071 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0521 07:51:00.790604 140049469417344 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 30.882s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ],
      "source": [
        "# From within TensorFlow/models/research/\n",
        "!python object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksTE2_JODyU3",
        "outputId": "a11dca3c-531b-46c8-cc02-46a04be67a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/training_demo/pre-trained-models\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/training_demo/pre-trained-models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw-Sh0M0DyXA",
        "outputId": "9b39b257-c0b7-4b01-96ee-61fceef8e59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-21 07:51:02--  http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.197.128, 2607:f8b0:400e:c05::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.197.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211996178 (202M) [application/x-tar]\n",
            "Saving to: ‘faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz.2’\n",
            "\n",
            "faster_rcnn_resnet5 100%[===================>] 202.17M  84.6MB/s    in 2.4s    \n",
            "\n",
            "2022-05-21 07:51:05 (84.6 MB/s) - ‘faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz.2’ saved [211996178/211996178]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyFEX5dMDyZI",
        "outputId": "68b80c74-9599-437a-821b-56da95be6be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint/\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint/checkpoint\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0.index\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/pipeline.config\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model/\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model/saved_model.pb\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model/variables/\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
            "faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model/variables/variables.index\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVAjqMoAIFJ3",
        "outputId": "2b21b6ac-f906-4969-e971-70d510ac46bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/training_demo\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/training_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcXdBmX-IFN9",
        "outputId": "2336d642-4039-4266-c146-171596d931dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34mannotations\u001b[0m/          export_tflite_graph_tf2.py   \u001b[01;34mpre-trained-models\u001b[0m/\n",
            " \u001b[01;34mcocoapi\u001b[0m/              generate_tfrecord.py         untitled\n",
            " \u001b[01;34meval\u001b[0m/                 \u001b[01;34mimages\u001b[0m/                     \u001b[01;34m'Untitled Folder'\u001b[0m/\n",
            " \u001b[01;34mexported-models\u001b[0m/      model_main_tf2.py\n",
            " exporter_main_v2.py   \u001b[01;34mmodels\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH4blMipIFQN",
        "outputId": "72ed5406-6711-4e10-bb4c-94d0459aa93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecord file: /content/drive/MyDrive/training_demo/annotations/train.record\n"
          ]
        }
      ],
      "source": [
        "!python generate_tfrecord.py -x /content/drive/MyDrive/training_demo/images/train -l /content/drive/MyDrive/training_demo/annotations/label_map.pbtxt -o /content/drive/MyDrive/training_demo/annotations/train.record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFPpmIUuIFdz",
        "outputId": "de96f53d-4ff8-4d69-b5b2-c404e289e55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecord file: /content/drive/MyDrive/training_demo/annotations/test.record\n"
          ]
        }
      ],
      "source": [
        "!python generate_tfrecord.py -x /content/drive/MyDrive/training_demo/images/test -l /content/drive/MyDrive/training_demo/annotations/label_map.pbtxt -o /content/drive/MyDrive/training_demo/annotations/test.record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdsR7bxFR5h8",
        "outputId": "6411f2fb-f0ff-48e2-edce-6f0c240779da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opencv-python-headless 4.5.5.64\n",
            "Uninstalling opencv-python-headless-4.5.5.64:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/*\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless-4.5.5.64.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libavcodec-65fa80df.so.58.134.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libavformat-8ef5c7db.so.58.76.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libavutil-9c768859.so.56.70.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libbz2-a273e504.so.1.0.6\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libcrypto-09fe7800.so.1.1\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libgfortran-91cc3cb1.so.3.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libopenblas-r0-f650aae0.3.3.so\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libssl-b92f8066.so.1.1\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libswresample-99364a1c.so.3.9.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libswscale-e6451464.so.5.9.100\n",
            "    /usr/local/lib/python3.7/dist-packages/opencv_python_headless.libs/libvpx-1016051d.so.7.0.0\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libQtCore-bbdab771.so.4.8.7\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libQtGui-903938cd.so.4.8.7\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libQtTest-1183da5d.so.4.8.7\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libavcodec-3cdd3bd4.so.58.62.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libavformat-69a63b50.so.58.35.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libavutil-8e8979a8.so.56.36.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libbz2-7225278b.so.1.0.3\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libcrypto-a25ff511.so.1.1\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libssl-fdf0b66c.so.1.1\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libswresample-c6b3bbb9.so.3.6.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libswscale-2d19f7d1.so.5.6.100\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libvpx-c887ea55.so.6.1.0\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/.libs/libz-a147dcb0.so.1.2.3\n",
            "    /usr/local/lib/python3.7/dist-packages/cv2/cv2.cpython-37m-x86_64-linux-gnu.so\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled opencv-python-headless-4.5.5.64\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall opencv-python-headless==4.5.5.62"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOGKwnAsR5mM",
        "outputId": "89cef6b9-d859-40f9-8d15-b88ca46d7df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless==4.1.2.30\n",
            "  Downloading opencv_python_headless-4.1.2.30-cp37-cp37m-manylinux1_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 145 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.1.2.30) (1.21.6)\n",
            "Installing collected packages: opencv-python-headless\n",
            "Successfully installed opencv-python-headless-4.1.2.30\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python-headless==4.1.2.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fothym-JGtp_",
        "outputId": "d7f455ce-e50a-4c7e-a014-4e375fb1d530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 40 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 3,139 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 12s (35.4 MB/s)\n",
            "(Reading database ... 155629 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 155607 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"
          ]
        }
      ],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wufoL_RIvtL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY8RYwIEITnR",
        "outputId": "239e2b99-f7a4-422b-8b77-4f0d1d8b8d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.9.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n",
            "2022-05-21 07:53:26.372906: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I0521 07:53:26.514134 140443699361664 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I0521 07:53:26.519678 140443699361664 config_util.py:552] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0521 07:53:26.519837 140443699361664 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0521 07:53:26.551384 140443699361664 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/drive/MyDrive/training_demo/annotations/train.record']\n",
            "I0521 07:53:26.560142 140443699361664 dataset_builder.py:162] Reading unweighted datasets: ['/content/drive/MyDrive/training_demo/annotations/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/drive/MyDrive/training_demo/annotations/train.record']\n",
            "I0521 07:53:26.560619 140443699361664 dataset_builder.py:79] Reading record datasets for input file: ['/content/drive/MyDrive/training_demo/annotations/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0521 07:53:26.560734 140443699361664 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0521 07:53:26.560806 140443699361664 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W0521 07:53:26.564031 140443699361664 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0521 07:53:26.586056 140443699361664 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/function.py\", line 968, in func_graph_from_py_func\n",
            "    outputs = func(*func_graph.inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\", line 196, in wrapped_fn\n",
            "    ret = wrapper_helper(*args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\", line 177, in wrapper_helper\n",
            "    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 689, in wrapper\n",
            "    return converted_call(f, args, kwargs, options=options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\n",
            "    result = converted_f(*effective_args, **kwargs)\n",
            "  File \"/tmp/__autograph_generated_filenqd28gji.py\", line 15, in tf__decode\n",
            "    tensors = ag__.converted_call(ag__.ld(decoder).decode, (ag__.ld(serialized_example),), dict(items=ag__.ld(keys)), fscope)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 427, in converted_call\n",
            "    converted_f = _convert_actual(target_entity, program_ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 269, in _convert_actual\n",
            "    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 282, in transform\n",
            "    return self.transform_function(obj, user_context)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 487, in transform_function\n",
            "    nodes, ctx.namer, future_features=ctx.info.future_features)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 180, in create\n",
            "    nodes, include_source_map=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/loader.py\", line 97, in load_ast\n",
            "    source_map = origin_info.create_source_map(nodes, source, module.__file__)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/origin_info.py\", line 110, in create_source_map\n",
            "    origin_info = anno.getanno(before, anno.Basic.ORIGIN, default=None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/anno.py\", line 124, in getanno\n",
            "    if (default is FAIL or (hasattr(node, field_name) and\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"model_main_tf2.py\", line 113, in <module>\n",
            "    tf.compat.v1.app.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 312, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 258, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"model_main_tf2.py\", line 110, in main\n",
            "    record_summaries=FLAGS.record_summaries)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 564, in train_loop\n",
            "    train_dataset_fn)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1195, in experimental_distribute_datasets_from_function\n",
            "    return self.distribute_datasets_from_function(dataset_fn, options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1187, in distribute_datasets_from_function\n",
            "    dataset_fn, options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 595, in _distribute_datasets_from_function\n",
            "    options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_util.py\", line 138, in get_distributed_datasets_from_function\n",
            "    build=build,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1372, in __init__\n",
            "    self.build()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1394, in build\n",
            "    self._input_contexts, self._input_workers, self._dataset_fn))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1875, in _create_datasets_from_function_with_input_context\n",
            "    dataset = dataset_fn(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 559, in train_dataset_fn\n",
            "    input_context=input_context)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/inputs.py\", line 913, in train_input\n",
            "    reduce_to_frame_fn=reduce_to_frame_fn)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py\", line 251, in build\n",
            "    input_reader_config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py\", line 236, in dataset_map_fn\n",
            "    fn_to_map, num_parallel_calls=num_parallel_calls)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\", line 357, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3920, in map_with_legacy_function\n",
            "    use_legacy_function=True))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 5288, in __init__\n",
            "    use_legacy_function=use_legacy_function)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\", line 278, in __init__\n",
            "    self._function.add_to_graph(ops.get_default_graph())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/function.py\", line 542, in add_to_graph\n",
            "    self._create_definition_if_needed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/function.py\", line 377, in _create_definition_if_needed\n",
            "    self._create_definition_if_needed_impl()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/function.py\", line 404, in _create_definition_if_needed_impl\n",
            "    capture_resource_var_by_value=self._capture_resource_var_by_value)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/function.py\", line 990, in func_graph_from_py_func\n",
            "    func_graph.outputs = outputs\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python model_main_tf2.py --model_dir=/content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50 --pipeline_config_path=/content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50/pipeline.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBUFmTfoFeCT",
        "outputId": "abd13855-4ee3-4fc9-e99b-1bce774c9115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"exporter_main_v2.py\", line 100, in <module>\n",
            "    from object_detection import exporter_lib_v2\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/exporter_lib_v2.py\", line 21, in <module>\n",
            "    from object_detection.builders import model_builder\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/builders/model_builder.py\", line 40, in <module>\n",
            "    from object_detection.meta_architectures import ssd_meta_arch\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/ssd_meta_arch.py\", line 32, in <module>\n",
            "    from object_detection.utils import visualization_utils\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/utils/visualization_utils.py\", line 29, in <module>\n",
            "    import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\", line 296, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\", line 358, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\", line 1280, in use\n",
            "    from matplotlib import pyplot as plt\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\", line 32, in <module>\n",
            "    import matplotlib.colorbar\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/colorbar.py\", line 31, in <module>\n",
            "    import matplotlib.contour as contour\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/contour.py\", line 17, in <module>\n",
            "    import matplotlib.text as text\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/text.py\", line 17, in <module>\n",
            "    from .textpath import TextPath  # Unused, but imported by others.\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/textpath.py\", line 11, in <module>\n",
            "    from matplotlib.mathtext import MathTextParser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/mathtext.py\", line 2237, in <module>\n",
            "    class Parser:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/mathtext.py\", line 2824, in Parser\n",
            "    set(_accent_map))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/mathtext.py\", line 2822, in <lambda>\n",
            "    _snowflake = (lambda am: [p for p in tex2uni if\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/mathtext.py\", line 2823, in <listcomp>\n",
            "    any(p.startswith(a) and a != p for a in am)])(\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/mathtext.py\", line 2823, in <genexpr>\n",
            "    any(p.startswith(a) and a != p for a in am)])(\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path /content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50/pipeline.config --trained_checkpoint_dir /content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50 --output_directory /content/drive/MyDrive/training_demo/exported-models/my_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"A set of functions that are used for visualization.\n",
        "\n",
        "These functions often receive an image, perform some visualization on the image.\n",
        "The functions do not return a value, instead they modify the image itself.\n",
        "\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import collections\n",
        "# Set headless-friendly backend.\n",
        "import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements\n",
        "import matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top\n",
        "import numpy as np\n",
        "import PIL.Image as Image\n",
        "import PIL.ImageColor as ImageColor\n",
        "import PIL.ImageDraw as ImageDraw\n",
        "import PIL.ImageFont as ImageFont\n",
        "import six\n",
        "from six.moves import range\n",
        "from six.moves import zip\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from object_detection.core import keypoint_ops\n",
        "from object_detection.core import standard_fields as fields\n",
        "from object_detection.utils import shape_utils\n",
        "\n",
        "_TITLE_LEFT_MARGIN = 10\n",
        "_TITLE_TOP_MARGIN = 10\n",
        "STANDARD_COLORS = [\n",
        "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
        "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
        "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
        "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
        "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
        "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
        "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
        "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
        "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
        "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
        "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
        "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
        "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
        "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
        "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
        "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
        "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
        "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
        "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
        "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
        "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
        "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
        "    'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
        "]\n",
        "\n",
        "\n",
        "def _get_multiplier_for_color_randomness():\n",
        "  \"\"\"Returns a multiplier to get semi-random colors from successive indices.\n",
        "\n",
        "  This function computes a prime number, p, in the range [2, 17] that:\n",
        "  - is closest to len(STANDARD_COLORS) / 10\n",
        "  - does not divide len(STANDARD_COLORS)\n",
        "\n",
        "  If no prime numbers in that range satisfy the constraints, p is returned as 1.\n",
        "\n",
        "  Once p is established, it can be used as a multiplier to select\n",
        "  non-consecutive colors from STANDARD_COLORS:\n",
        "  colors = [(p * i) % len(STANDARD_COLORS) for i in range(20)]\n",
        "  \"\"\"\n",
        "  num_colors = len(STANDARD_COLORS)\n",
        "  prime_candidates = [5, 7, 11, 13, 17]\n",
        "\n",
        "  # Remove all prime candidates that divide the number of colors.\n",
        "  prime_candidates = [p for p in prime_candidates if num_colors % p]\n",
        "  if not prime_candidates:\n",
        "    return 1\n",
        "\n",
        "  # Return the closest prime number to num_colors / 10.\n",
        "  abs_distance = [np.abs(num_colors / 10. - p) for p in prime_candidates]\n",
        "  num_candidates = len(abs_distance)\n",
        "  inds = [i for _, i in sorted(zip(abs_distance, range(num_candidates)))]\n",
        "  return prime_candidates[inds[0]]\n",
        "\n",
        "\n",
        "def save_image_array_as_png(image, output_path):\n",
        "  \"\"\"Saves an image (represented as a numpy array) to PNG.\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array with shape [height, width, 3].\n",
        "    output_path: path to which image should be written.\n",
        "  \"\"\"\n",
        "  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n",
        "  with tf.gfile.Open(output_path, 'w') as fid:\n",
        "    image_pil.save(fid, 'PNG')\n",
        "\n",
        "\n",
        "def encode_image_array_as_png_str(image):\n",
        "  \"\"\"Encodes a numpy array into a PNG string.\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array with shape [height, width, 3].\n",
        "\n",
        "  Returns:\n",
        "    PNG encoded image string.\n",
        "  \"\"\"\n",
        "  image_pil = Image.fromarray(np.uint8(image))\n",
        "  output = six.BytesIO()\n",
        "  image_pil.save(output, format='PNG')\n",
        "  png_string = output.getvalue()\n",
        "  output.close()\n",
        "  return png_string\n",
        "\n",
        "\n",
        "def draw_bounding_box_on_image_array(image,\n",
        "                                     ymin,\n",
        "                                     xmin,\n",
        "                                     ymax,\n",
        "                                     xmax,\n",
        "                                     color='red',\n",
        "                                     thickness=4,\n",
        "                                     display_str_list=(),\n",
        "                                     use_normalized_coordinates=True):\n",
        "  \"\"\"Adds a bounding box to an image (numpy array).\n",
        "\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array with shape [height, width, 3].\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: list of strings to display in box\n",
        "                      (each to be shown on its own line).\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n",
        "  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color,\n",
        "                             thickness, display_str_list,\n",
        "                             use_normalized_coordinates)\n",
        "  np.copyto(image, np.array(image_pil))\n",
        "\n",
        "\n",
        "def draw_bounding_box_on_image(image,\n",
        "                               ymin,\n",
        "                               xmin,\n",
        "                               ymax,\n",
        "                               xmax,\n",
        "                               color='red',\n",
        "                               thickness=4,\n",
        "                               display_str_list=(),\n",
        "                               use_normalized_coordinates=True):\n",
        "  \"\"\"Adds a bounding box to an image.\n",
        "\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "\n",
        "  Each string in display_str_list is displayed on a separate line above the\n",
        "  bounding box in black text on a rectangle filled with the input 'color'.\n",
        "  If the top of the bounding box extends to the edge of the image, the strings\n",
        "  are displayed below the bounding box.\n",
        "\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: list of strings to display in box\n",
        "                      (each to be shown on its own line).\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  if use_normalized_coordinates:\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                  ymin * im_height, ymax * im_height)\n",
        "  else:\n",
        "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
        "  if thickness > 0:\n",
        "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "               (left, top)],\n",
        "              width=thickness,\n",
        "              fill=color)\n",
        "  try:\n",
        "    font = ImageFont.truetype('arial.ttf', 24)\n",
        "  except IOError:\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "  # If the total height of the display strings added to the top of the bounding\n",
        "  # box exceeds the top of the image, stack the strings below the bounding box\n",
        "  # instead of above.\n",
        "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "  # Each display_str has a top and bottom margin of 0.05x.\n",
        "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "\n",
        "  if top > total_display_str_height:\n",
        "    text_bottom = top\n",
        "  else:\n",
        "    text_bottom = bottom + total_display_str_height\n",
        "  # Reverse list and print from bottom to top.\n",
        "  for display_str in display_str_list[::-1]:\n",
        "    text_width, text_height = font.getsize(display_str)\n",
        "    margin = np.ceil(0.05 * text_height)\n",
        "    draw.rectangle(\n",
        "        [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n",
        "                                                          text_bottom)],\n",
        "        fill=color)\n",
        "    draw.text(\n",
        "        (left + margin, text_bottom - text_height - margin),\n",
        "        display_str,\n",
        "        fill='black',\n",
        "        font=font)\n",
        "    text_bottom -= text_height - 2 * margin\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image_array(image,\n",
        "                                       boxes,\n",
        "                                       color='red',\n",
        "                                       thickness=4,\n",
        "                                       display_str_list_list=()):\n",
        "  \"\"\"Draws bounding boxes on image (numpy array).\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: list of list of strings.\n",
        "                           a list of strings for each bounding box.\n",
        "                           The reason to pass a list of strings for a\n",
        "                           bounding box is that it might contain\n",
        "                           multiple labels.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  image_pil = Image.fromarray(image)\n",
        "  draw_bounding_boxes_on_image(image_pil, boxes, color, thickness,\n",
        "                               display_str_list_list)\n",
        "  np.copyto(image, np.array(image_pil))\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image(image,\n",
        "                                 boxes,\n",
        "                                 color='red',\n",
        "                                 thickness=4,\n",
        "                                 display_str_list_list=()):\n",
        "  \"\"\"Draws bounding boxes on image.\n",
        "\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: list of list of strings.\n",
        "                           a list of strings for each bounding box.\n",
        "                           The reason to pass a list of strings for a\n",
        "                           bounding box is that it might contain\n",
        "                           multiple labels.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  boxes_shape = boxes.shape\n",
        "  if not boxes_shape:\n",
        "    return\n",
        "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "    raise ValueError('Input must be of size [N, 4]')\n",
        "  for i in range(boxes_shape[0]):\n",
        "    display_str_list = ()\n",
        "    if display_str_list_list:\n",
        "      display_str_list = display_str_list_list[i]\n",
        "    draw_bounding_box_on_image(image, boxes[i, 0], boxes[i, 1], boxes[i, 2],\n",
        "                               boxes[i, 3], color, thickness, display_str_list)\n",
        "\n",
        "\n",
        "def create_visualization_fn(category_index,\n",
        "                            include_masks=False,\n",
        "                            include_keypoints=False,\n",
        "                            include_keypoint_scores=False,\n",
        "                            include_track_ids=False,\n",
        "                            **kwargs):\n",
        "  \"\"\"Constructs a visualization function that can be wrapped in a py_func.\n",
        "\n",
        "  py_funcs only accept positional arguments. This function returns a suitable\n",
        "  function with the correct positional argument mapping. The positional\n",
        "  arguments in order are:\n",
        "  0: image\n",
        "  1: boxes\n",
        "  2: classes\n",
        "  3: scores\n",
        "  [4]: masks (optional)\n",
        "  [4-5]: keypoints (optional)\n",
        "  [4-6]: keypoint_scores (optional)\n",
        "  [4-7]: track_ids (optional)\n",
        "\n",
        "  -- Example 1 --\n",
        "  vis_only_masks_fn = create_visualization_fn(category_index,\n",
        "    include_masks=True, include_keypoints=False, include_track_ids=False,\n",
        "    **kwargs)\n",
        "  image = tf.py_func(vis_only_masks_fn,\n",
        "                     inp=[image, boxes, classes, scores, masks],\n",
        "                     Tout=tf.uint8)\n",
        "\n",
        "  -- Example 2 --\n",
        "  vis_masks_and_track_ids_fn = create_visualization_fn(category_index,\n",
        "    include_masks=True, include_keypoints=False, include_track_ids=True,\n",
        "    **kwargs)\n",
        "  image = tf.py_func(vis_masks_and_track_ids_fn,\n",
        "                     inp=[image, boxes, classes, scores, masks, track_ids],\n",
        "                     Tout=tf.uint8)\n",
        "\n",
        "  Args:\n",
        "    category_index: a dict that maps integer ids to category dicts. e.g.\n",
        "      {1: {1: 'dog'}, 2: {2: 'cat'}, ...}\n",
        "    include_masks: Whether masks should be expected as a positional argument in\n",
        "      the returned function.\n",
        "    include_keypoints: Whether keypoints should be expected as a positional\n",
        "      argument in the returned function.\n",
        "    include_keypoint_scores: Whether keypoint scores should be expected as a\n",
        "      positional argument in the returned function.\n",
        "    include_track_ids: Whether track ids should be expected as a positional\n",
        "      argument in the returned function.\n",
        "    **kwargs: Additional kwargs that will be passed to\n",
        "      visualize_boxes_and_labels_on_image_array.\n",
        "\n",
        "  Returns:\n",
        "    Returns a function that only takes tensors as positional arguments.\n",
        "  \"\"\"\n",
        "\n",
        "  def visualization_py_func_fn(*args):\n",
        "    \"\"\"Visualization function that can be wrapped in a tf.py_func.\n",
        "\n",
        "    Args:\n",
        "      *args: First 4 positional arguments must be:\n",
        "        image - uint8 numpy array with shape (img_height, img_width, 3).\n",
        "        boxes - a numpy array of shape [N, 4].\n",
        "        classes - a numpy array of shape [N].\n",
        "        scores - a numpy array of shape [N] or None.\n",
        "        -- Optional positional arguments --\n",
        "        instance_masks - a numpy array of shape [N, image_height, image_width].\n",
        "        keypoints - a numpy array of shape [N, num_keypoints, 2].\n",
        "        keypoint_scores - a numpy array of shape [N, num_keypoints].\n",
        "        track_ids - a numpy array of shape [N] with unique track ids.\n",
        "\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3) with overlaid\n",
        "      boxes.\n",
        "    \"\"\"\n",
        "    image = args[0]\n",
        "    boxes = args[1]\n",
        "    classes = args[2]\n",
        "    scores = args[3]\n",
        "    masks = keypoints = keypoint_scores = track_ids = None\n",
        "    pos_arg_ptr = 4  # Positional argument for first optional tensor (masks).\n",
        "    if include_masks:\n",
        "      masks = args[pos_arg_ptr]\n",
        "      pos_arg_ptr += 1\n",
        "    if include_keypoints:\n",
        "      keypoints = args[pos_arg_ptr]\n",
        "      pos_arg_ptr += 1\n",
        "    if include_keypoint_scores:\n",
        "      keypoint_scores = args[pos_arg_ptr]\n",
        "      pos_arg_ptr += 1\n",
        "    if include_track_ids:\n",
        "      track_ids = args[pos_arg_ptr]\n",
        "\n",
        "    return visualize_boxes_and_labels_on_image_array(\n",
        "        image,\n",
        "        boxes,\n",
        "        classes,\n",
        "        scores,\n",
        "        category_index=category_index,\n",
        "        instance_masks=masks,\n",
        "        keypoints=keypoints,\n",
        "        keypoint_scores=keypoint_scores,\n",
        "        track_ids=track_ids,\n",
        "        **kwargs)\n",
        "  return visualization_py_func_fn\n",
        "\n",
        "\n",
        "def draw_heatmaps_on_image(image, heatmaps):\n",
        "  \"\"\"Draws heatmaps on an image.\n",
        "\n",
        "  The heatmaps are handled channel by channel and different colors are used to\n",
        "  paint different heatmap channels.\n",
        "\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    heatmaps: a numpy array with shape [image_height, image_width, channel].\n",
        "      Note that the image_height and image_width should match the size of input\n",
        "      image.\n",
        "  \"\"\"\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  channel = heatmaps.shape[2]\n",
        "  for c in range(channel):\n",
        "    heatmap = heatmaps[:, :, c] * 255\n",
        "    heatmap = heatmap.astype('uint8')\n",
        "    bitmap = Image.fromarray(heatmap, 'L')\n",
        "    bitmap.convert('1')\n",
        "    draw.bitmap(\n",
        "        xy=[(0, 0)],\n",
        "        bitmap=bitmap,\n",
        "        fill=STANDARD_COLORS[c])\n",
        "\n",
        "\n",
        "def draw_heatmaps_on_image_array(image, heatmaps):\n",
        "  \"\"\"Overlays heatmaps to an image (numpy array).\n",
        "\n",
        "  The function overlays the heatmaps on top of image. The heatmap values will be\n",
        "  painted with different colors depending on the channels. Similar to\n",
        "  \"draw_heatmaps_on_image_array\" function except the inputs are numpy arrays.\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array with shape [height, width, 3].\n",
        "    heatmaps: a numpy array with shape [height, width, channel].\n",
        "\n",
        "  Returns:\n",
        "    An uint8 numpy array representing the input image painted with heatmap\n",
        "    colors.\n",
        "  \"\"\"\n",
        "  if not isinstance(image, np.ndarray):\n",
        "    image = image.numpy()\n",
        "  if not isinstance(heatmaps, np.ndarray):\n",
        "    heatmaps = heatmaps.numpy()\n",
        "  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n",
        "  draw_heatmaps_on_image(image_pil, heatmaps)\n",
        "  return np.array(image_pil)\n",
        "\n",
        "\n",
        "def draw_heatmaps_on_image_tensors(images,\n",
        "                                   heatmaps,\n",
        "                                   apply_sigmoid=False):\n",
        "  \"\"\"Draws heatmaps on batch of image tensors.\n",
        "\n",
        "  Args:\n",
        "    images: A 4D uint8 image tensor of shape [N, H, W, C]. If C > 3, additional\n",
        "      channels will be ignored. If C = 1, then we convert the images to RGB\n",
        "      images.\n",
        "    heatmaps: [N, h, w, channel] float32 tensor of heatmaps. Note that the\n",
        "      heatmaps will be resized to match the input image size before overlaying\n",
        "      the heatmaps with input images. Theoretically the heatmap height width\n",
        "      should have the same aspect ratio as the input image to avoid potential\n",
        "      misalignment introduced by the image resize.\n",
        "    apply_sigmoid: Whether to apply a sigmoid layer on top of the heatmaps. If\n",
        "      the heatmaps come directly from the prediction logits, then we should\n",
        "      apply the sigmoid layer to make sure the values are in between [0.0, 1.0].\n",
        "\n",
        "  Returns:\n",
        "    4D image tensor of type uint8, with heatmaps overlaid on top.\n",
        "  \"\"\"\n",
        "  # Additional channels are being ignored.\n",
        "  if images.shape[3] > 3:\n",
        "    images = images[:, :, :, 0:3]\n",
        "  elif images.shape[3] == 1:\n",
        "    images = tf.image.grayscale_to_rgb(images)\n",
        "\n",
        "  _, height, width, _ = shape_utils.combined_static_and_dynamic_shape(images)\n",
        "  if apply_sigmoid:\n",
        "    heatmaps = tf.math.sigmoid(heatmaps)\n",
        "  resized_heatmaps = tf.image.resize(heatmaps, size=[height, width])\n",
        "\n",
        "  elems = [images, resized_heatmaps]\n",
        "\n",
        "  def draw_heatmaps(image_and_heatmaps):\n",
        "    \"\"\"Draws heatmaps on image.\"\"\"\n",
        "    image_with_heatmaps = tf.py_function(\n",
        "        draw_heatmaps_on_image_array,\n",
        "        image_and_heatmaps,\n",
        "        tf.uint8)\n",
        "    return image_with_heatmaps\n",
        "  images = tf.map_fn(draw_heatmaps, elems, dtype=tf.uint8, back_prop=False)\n",
        "  return images\n",
        "\n",
        "\n",
        "def _resize_original_image(image, image_shape):\n",
        "  image = tf.expand_dims(image, 0)\n",
        "  image = tf.image.resize_images(\n",
        "      image,\n",
        "      image_shape,\n",
        "      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
        "      align_corners=True)\n",
        "  return tf.cast(tf.squeeze(image, 0), tf.uint8)\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image_tensors(images,\n",
        "                                         boxes,\n",
        "                                         classes,\n",
        "                                         scores,\n",
        "                                         category_index,\n",
        "                                         original_image_spatial_shape=None,\n",
        "                                         true_image_shape=None,\n",
        "                                         instance_masks=None,\n",
        "                                         keypoints=None,\n",
        "                                         keypoint_scores=None,\n",
        "                                         keypoint_edges=None,\n",
        "                                         track_ids=None,\n",
        "                                         max_boxes_to_draw=20,\n",
        "                                         min_score_thresh=0.2,\n",
        "                                         use_normalized_coordinates=True):\n",
        "  \"\"\"Draws bounding boxes, masks, and keypoints on batch of image tensors.\n",
        "\n",
        "  Args:\n",
        "    images: A 4D uint8 image tensor of shape [N, H, W, C]. If C > 3, additional\n",
        "      channels will be ignored. If C = 1, then we convert the images to RGB\n",
        "      images.\n",
        "    boxes: [N, max_detections, 4] float32 tensor of detection boxes.\n",
        "    classes: [N, max_detections] int tensor of detection classes. Note that\n",
        "      classes are 1-indexed.\n",
        "    scores: [N, max_detections] float32 tensor of detection scores.\n",
        "    category_index: a dict that maps integer ids to category dicts. e.g.\n",
        "      {1: {1: 'dog'}, 2: {2: 'cat'}, ...}\n",
        "    original_image_spatial_shape: [N, 2] tensor containing the spatial size of\n",
        "      the original image.\n",
        "    true_image_shape: [N, 3] tensor containing the spatial size of unpadded\n",
        "      original_image.\n",
        "    instance_masks: A 4D uint8 tensor of shape [N, max_detection, H, W] with\n",
        "      instance masks.\n",
        "    keypoints: A 4D float32 tensor of shape [N, max_detection, num_keypoints, 2]\n",
        "      with keypoints.\n",
        "    keypoint_scores: A 3D float32 tensor of shape [N, max_detection,\n",
        "      num_keypoints] with keypoint scores.\n",
        "    keypoint_edges: A list of tuples with keypoint indices that specify which\n",
        "      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws\n",
        "      edges from keypoint 0 to 1 and from keypoint 2 to 4.\n",
        "    track_ids: [N, max_detections] int32 tensor of unique tracks ids (i.e.\n",
        "      instance ids for each object). If provided, the color-coding of boxes is\n",
        "      dictated by these ids, and not classes.\n",
        "    max_boxes_to_draw: Maximum number of boxes to draw on an image. Default 20.\n",
        "    min_score_thresh: Minimum score threshold for visualization. Default 0.2.\n",
        "    use_normalized_coordinates: Whether to assume boxes and kepoints are in\n",
        "      normalized coordinates (as opposed to absolute coordiantes).\n",
        "      Default is True.\n",
        "\n",
        "  Returns:\n",
        "    4D image tensor of type uint8, with boxes drawn on top.\n",
        "  \"\"\"\n",
        "  # Additional channels are being ignored.\n",
        "  if images.shape[3] > 3:\n",
        "    images = images[:, :, :, 0:3]\n",
        "  elif images.shape[3] == 1:\n",
        "    images = tf.image.grayscale_to_rgb(images)\n",
        "  visualization_keyword_args = {\n",
        "      'use_normalized_coordinates': use_normalized_coordinates,\n",
        "      'max_boxes_to_draw': max_boxes_to_draw,\n",
        "      'min_score_thresh': min_score_thresh,\n",
        "      'agnostic_mode': False,\n",
        "      'line_thickness': 4,\n",
        "      'keypoint_edges': keypoint_edges\n",
        "  }\n",
        "  if true_image_shape is None:\n",
        "    true_shapes = tf.constant(-1, shape=[images.shape.as_list()[0], 3])\n",
        "  else:\n",
        "    true_shapes = true_image_shape\n",
        "  if original_image_spatial_shape is None:\n",
        "    original_shapes = tf.constant(-1, shape=[images.shape.as_list()[0], 2])\n",
        "  else:\n",
        "    original_shapes = original_image_spatial_shape\n",
        "\n",
        "  visualize_boxes_fn = create_visualization_fn(\n",
        "      category_index,\n",
        "      include_masks=instance_masks is not None,\n",
        "      include_keypoints=keypoints is not None,\n",
        "      include_keypoint_scores=keypoint_scores is not None,\n",
        "      include_track_ids=track_ids is not None,\n",
        "      **visualization_keyword_args)\n",
        "\n",
        "  elems = [true_shapes, original_shapes, images, boxes, classes, scores]\n",
        "  if instance_masks is not None:\n",
        "    elems.append(instance_masks)\n",
        "  if keypoints is not None:\n",
        "    elems.append(keypoints)\n",
        "  if keypoint_scores is not None:\n",
        "    elems.append(keypoint_scores)\n",
        "  if track_ids is not None:\n",
        "    elems.append(track_ids)\n",
        "\n",
        "  def draw_boxes(image_and_detections):\n",
        "    \"\"\"Draws boxes on image.\"\"\"\n",
        "    true_shape = image_and_detections[0]\n",
        "    original_shape = image_and_detections[1]\n",
        "    if true_image_shape is not None:\n",
        "      image = shape_utils.pad_or_clip_nd(image_and_detections[2],\n",
        "                                         [true_shape[0], true_shape[1], 3])\n",
        "    if original_image_spatial_shape is not None:\n",
        "      image_and_detections[2] = _resize_original_image(image, original_shape)\n",
        "\n",
        "    image_with_boxes = tf.py_func(visualize_boxes_fn, image_and_detections[2:],\n",
        "                                  tf.uint8)\n",
        "    return image_with_boxes\n",
        "\n",
        "  images = tf.map_fn(draw_boxes, elems, dtype=tf.uint8, back_prop=False)\n",
        "  return images\n",
        "\n",
        "\n",
        "def draw_side_by_side_evaluation_image(eval_dict,\n",
        "                                       category_index,\n",
        "                                       max_boxes_to_draw=20,\n",
        "                                       min_score_thresh=0.2,\n",
        "                                       use_normalized_coordinates=True,\n",
        "                                       keypoint_edges=None):\n",
        "  \"\"\"Creates a side-by-side image with detections and groundtruth.\n",
        "\n",
        "  Bounding boxes (and instance masks, if available) are visualized on both\n",
        "  subimages.\n",
        "\n",
        "  Args:\n",
        "    eval_dict: The evaluation dictionary returned by\n",
        "      eval_util.result_dict_for_batched_example() or\n",
        "      eval_util.result_dict_for_single_example().\n",
        "    category_index: A category index (dictionary) produced from a labelmap.\n",
        "    max_boxes_to_draw: The maximum number of boxes to draw for detections.\n",
        "    min_score_thresh: The minimum score threshold for showing detections.\n",
        "    use_normalized_coordinates: Whether to assume boxes and keypoints are in\n",
        "      normalized coordinates (as opposed to absolute coordinates).\n",
        "      Default is True.\n",
        "    keypoint_edges: A list of tuples with keypoint indices that specify which\n",
        "      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws\n",
        "      edges from keypoint 0 to 1 and from keypoint 2 to 4.\n",
        "\n",
        "  Returns:\n",
        "    A list of [1, H, 2 * W, C] uint8 tensor. The subimage on the left\n",
        "      corresponds to detections, while the subimage on the right corresponds to\n",
        "      groundtruth.\n",
        "  \"\"\"\n",
        "  detection_fields = fields.DetectionResultFields()\n",
        "  input_data_fields = fields.InputDataFields()\n",
        "\n",
        "  images_with_detections_list = []\n",
        "\n",
        "  # Add the batch dimension if the eval_dict is for single example.\n",
        "  if len(eval_dict[detection_fields.detection_classes].shape) == 1:\n",
        "    for key in eval_dict:\n",
        "      if (key != input_data_fields.original_image and\n",
        "          key != input_data_fields.image_additional_channels):\n",
        "        eval_dict[key] = tf.expand_dims(eval_dict[key], 0)\n",
        "\n",
        "  num_gt_boxes = [-1] * eval_dict[input_data_fields.original_image].shape[0]\n",
        "  if input_data_fields.num_groundtruth_boxes in eval_dict:\n",
        "    num_gt_boxes = tf.cast(eval_dict[input_data_fields.num_groundtruth_boxes],\n",
        "                           tf.int32)\n",
        "  for indx in range(eval_dict[input_data_fields.original_image].shape[0]):\n",
        "    instance_masks = None\n",
        "    if detection_fields.detection_masks in eval_dict:\n",
        "      instance_masks = tf.cast(\n",
        "          tf.expand_dims(\n",
        "              eval_dict[detection_fields.detection_masks][indx], axis=0),\n",
        "          tf.uint8)\n",
        "    keypoints = None\n",
        "    keypoint_scores = None\n",
        "    if detection_fields.detection_keypoints in eval_dict:\n",
        "      keypoints = tf.expand_dims(\n",
        "          eval_dict[detection_fields.detection_keypoints][indx], axis=0)\n",
        "      if detection_fields.detection_keypoint_scores in eval_dict:\n",
        "        keypoint_scores = tf.expand_dims(\n",
        "            eval_dict[detection_fields.detection_keypoint_scores][indx], axis=0)\n",
        "      else:\n",
        "        keypoint_scores = tf.expand_dims(tf.cast(\n",
        "            keypoint_ops.set_keypoint_visibilities(\n",
        "                eval_dict[detection_fields.detection_keypoints][indx]),\n",
        "            dtype=tf.float32), axis=0)\n",
        "\n",
        "    groundtruth_instance_masks = None\n",
        "    if input_data_fields.groundtruth_instance_masks in eval_dict:\n",
        "      groundtruth_instance_masks = tf.cast(\n",
        "          tf.expand_dims(\n",
        "              eval_dict[input_data_fields.groundtruth_instance_masks][indx],\n",
        "              axis=0), tf.uint8)\n",
        "    groundtruth_keypoints = None\n",
        "    groundtruth_keypoint_scores = None\n",
        "    gt_kpt_vis_fld = input_data_fields.groundtruth_keypoint_visibilities\n",
        "    if input_data_fields.groundtruth_keypoints in eval_dict:\n",
        "      groundtruth_keypoints = tf.expand_dims(\n",
        "          eval_dict[input_data_fields.groundtruth_keypoints][indx], axis=0)\n",
        "      if gt_kpt_vis_fld in eval_dict:\n",
        "        groundtruth_keypoint_scores = tf.expand_dims(\n",
        "            tf.cast(eval_dict[gt_kpt_vis_fld][indx], dtype=tf.float32), axis=0)\n",
        "      else:\n",
        "        groundtruth_keypoint_scores = tf.expand_dims(tf.cast(\n",
        "            keypoint_ops.set_keypoint_visibilities(\n",
        "                eval_dict[input_data_fields.groundtruth_keypoints][indx]),\n",
        "            dtype=tf.float32), axis=0)\n",
        "    images_with_detections = draw_bounding_boxes_on_image_tensors(\n",
        "        tf.expand_dims(\n",
        "            eval_dict[input_data_fields.original_image][indx], axis=0),\n",
        "        tf.expand_dims(\n",
        "            eval_dict[detection_fields.detection_boxes][indx], axis=0),\n",
        "        tf.expand_dims(\n",
        "            eval_dict[detection_fields.detection_classes][indx], axis=0),\n",
        "        tf.expand_dims(\n",
        "            eval_dict[detection_fields.detection_scores][indx], axis=0),\n",
        "        category_index,\n",
        "        original_image_spatial_shape=tf.expand_dims(\n",
        "            eval_dict[input_data_fields.original_image_spatial_shape][indx],\n",
        "            axis=0),\n",
        "        true_image_shape=tf.expand_dims(\n",
        "            eval_dict[input_data_fields.true_image_shape][indx], axis=0),\n",
        "        instance_masks=instance_masks,\n",
        "        keypoints=keypoints,\n",
        "        keypoint_scores=keypoint_scores,\n",
        "        keypoint_edges=keypoint_edges,\n",
        "        max_boxes_to_draw=max_boxes_to_draw,\n",
        "        min_score_thresh=min_score_thresh,\n",
        "        use_normalized_coordinates=use_normalized_coordinates)\n",
        "    num_gt_boxes_i = num_gt_boxes[indx]\n",
        "    images_with_groundtruth = draw_bounding_boxes_on_image_tensors(\n",
        "        tf.expand_dims(\n",
        "            eval_dict[input_data_fields.original_image][indx],\n",
        "            axis=0),\n",
        "        tf.expand_dims(\n",
        "            eval_dict[input_data_fields.groundtruth_boxes][indx]\n",
        "            [:num_gt_boxes_i],\n",
        "            axis=0),\n",
        "        tf.expand_dims(\n",
        "            eval_dict[input_data_fields.groundtruth_classes][indx]\n",
        "            [:num_gt_boxes_i],\n",
        "            axis=0),\n",
        "        tf.expand_dims(\n",
        "            tf.ones_like(\n",
        "                eval_dict[input_data_fields.groundtruth_classes][indx]\n",
        "                [:num_gt_boxes_i],\n",
        "                dtype=tf.float32),\n",
        "            axis=0),\n",
        "        category_index,\n",
        "        original_image_spatial_shape=tf.expand_dims(\n",
        "            eval_dict[input_data_fields.original_image_spatial_shape][indx],\n",
        "            axis=0),\n",
        "        true_image_shape=tf.expand_dims(\n",
        "            eval_dict[input_data_fields.true_image_shape][indx], axis=0),\n",
        "        instance_masks=groundtruth_instance_masks,\n",
        "        keypoints=groundtruth_keypoints,\n",
        "        keypoint_scores=groundtruth_keypoint_scores,\n",
        "        keypoint_edges=keypoint_edges,\n",
        "        max_boxes_to_draw=None,\n",
        "        min_score_thresh=0.0,\n",
        "        use_normalized_coordinates=use_normalized_coordinates)\n",
        "    images_to_visualize = tf.concat([images_with_detections,\n",
        "                                     images_with_groundtruth], axis=2)\n",
        "\n",
        "    if input_data_fields.image_additional_channels in eval_dict:\n",
        "      images_with_additional_channels_groundtruth = (\n",
        "          draw_bounding_boxes_on_image_tensors(\n",
        "              tf.expand_dims(\n",
        "                  eval_dict[input_data_fields.image_additional_channels][indx],\n",
        "                  axis=0),\n",
        "              tf.expand_dims(\n",
        "                  eval_dict[input_data_fields.groundtruth_boxes][indx]\n",
        "                  [:num_gt_boxes_i],\n",
        "                  axis=0),\n",
        "              tf.expand_dims(\n",
        "                  eval_dict[input_data_fields.groundtruth_classes][indx]\n",
        "                  [:num_gt_boxes_i],\n",
        "                  axis=0),\n",
        "              tf.expand_dims(\n",
        "                  tf.ones_like(\n",
        "                      eval_dict[input_data_fields.groundtruth_classes][indx]\n",
        "                      [num_gt_boxes_i],\n",
        "                      dtype=tf.float32),\n",
        "                  axis=0),\n",
        "              category_index,\n",
        "              original_image_spatial_shape=tf.expand_dims(\n",
        "                  eval_dict[input_data_fields.original_image_spatial_shape]\n",
        "                  [indx],\n",
        "                  axis=0),\n",
        "              true_image_shape=tf.expand_dims(\n",
        "                  eval_dict[input_data_fields.true_image_shape][indx], axis=0),\n",
        "              instance_masks=groundtruth_instance_masks,\n",
        "              keypoints=None,\n",
        "              keypoint_edges=None,\n",
        "              max_boxes_to_draw=None,\n",
        "              min_score_thresh=0.0,\n",
        "              use_normalized_coordinates=use_normalized_coordinates))\n",
        "      images_to_visualize = tf.concat(\n",
        "          [images_to_visualize, images_with_additional_channels_groundtruth],\n",
        "          axis=2)\n",
        "    images_with_detections_list.append(images_to_visualize)\n",
        "\n",
        "  return images_with_detections_list\n",
        "\n",
        "\n",
        "def draw_densepose_visualizations(eval_dict,\n",
        "                                  max_boxes_to_draw=20,\n",
        "                                  min_score_thresh=0.2,\n",
        "                                  num_parts=24,\n",
        "                                  dp_coord_to_visualize=0):\n",
        "  \"\"\"Draws DensePose visualizations.\n",
        "\n",
        "  Args:\n",
        "    eval_dict: The evaluation dictionary returned by\n",
        "      eval_util.result_dict_for_batched_example().\n",
        "    max_boxes_to_draw: The maximum number of boxes to draw for detections.\n",
        "    min_score_thresh: The minimum score threshold for showing detections.\n",
        "    num_parts: The number of different densepose parts.\n",
        "    dp_coord_to_visualize: Whether to visualize v-coordinates (0) or\n",
        "      u-coordinates (0) overlaid on the person masks.\n",
        "\n",
        "  Returns:\n",
        "    A list of [1, H, W, C] uint8 tensor, each element corresponding to an image\n",
        "    in the batch.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `dp_coord_to_visualize` is not 0 or 1.\n",
        "  \"\"\"\n",
        "  if dp_coord_to_visualize not in (0, 1):\n",
        "    raise ValueError('`dp_coord_to_visualize` must be either 0 for v '\n",
        "                     'coordinates), or 1 for u coordinates, but instead got '\n",
        "                     '{}'.format(dp_coord_to_visualize))\n",
        "  detection_fields = fields.DetectionResultFields()\n",
        "  input_data_fields = fields.InputDataFields()\n",
        "\n",
        "  if detection_fields.detection_masks not in eval_dict:\n",
        "    raise ValueError('Expected `detection_masks` in `eval_dict`.')\n",
        "  if detection_fields.detection_surface_coords not in eval_dict:\n",
        "    raise ValueError('Expected `detection_surface_coords` in `eval_dict`.')\n",
        "\n",
        "  images_with_detections_list = []\n",
        "  for indx in range(eval_dict[input_data_fields.original_image].shape[0]):\n",
        "    # Note that detection masks have already been resized to the original image\n",
        "    # shapes, but `original_image` has not.\n",
        "    # TODO(ronnyvotel): Consider resizing `original_image` in\n",
        "    # eval_util.result_dict_for_batched_example().\n",
        "    true_shape = eval_dict[input_data_fields.true_image_shape][indx]\n",
        "    original_shape = eval_dict[\n",
        "        input_data_fields.original_image_spatial_shape][indx]\n",
        "    image = eval_dict[input_data_fields.original_image][indx]\n",
        "    image = shape_utils.pad_or_clip_nd(image, [true_shape[0], true_shape[1], 3])\n",
        "    image = _resize_original_image(image, original_shape)\n",
        "\n",
        "    scores = eval_dict[detection_fields.detection_scores][indx]\n",
        "    detection_masks = eval_dict[detection_fields.detection_masks][indx]\n",
        "    surface_coords = eval_dict[detection_fields.detection_surface_coords][indx]\n",
        "\n",
        "    def draw_densepose_py_func(image, detection_masks, surface_coords, scores):\n",
        "      \"\"\"Overlays part masks and surface coords on original images.\"\"\"\n",
        "      surface_coord_image = np.copy(image)\n",
        "      for i, (score, surface_coord, mask) in enumerate(\n",
        "          zip(scores, surface_coords, detection_masks)):\n",
        "        if i == max_boxes_to_draw:\n",
        "          break\n",
        "        if score > min_score_thresh:\n",
        "          draw_part_mask_on_image_array(image, mask, num_parts=num_parts)\n",
        "          draw_float_channel_on_image_array(\n",
        "              surface_coord_image, surface_coord[:, :, dp_coord_to_visualize],\n",
        "              mask)\n",
        "      return np.concatenate([image, surface_coord_image], axis=1)\n",
        "\n",
        "    image_with_densepose = tf.py_func(\n",
        "        draw_densepose_py_func,\n",
        "        [image, detection_masks, surface_coords, scores],\n",
        "        tf.uint8)\n",
        "    images_with_detections_list.append(\n",
        "        image_with_densepose[tf.newaxis, :, :, :])\n",
        "  return images_with_detections_list\n",
        "\n",
        "\n",
        "def draw_keypoints_on_image_array(image,\n",
        "                                  keypoints,\n",
        "                                  keypoint_scores=None,\n",
        "                                  min_score_thresh=0.5,\n",
        "                                  color='red',\n",
        "                                  radius=2,\n",
        "                                  use_normalized_coordinates=True,\n",
        "                                  keypoint_edges=None,\n",
        "                                  keypoint_edge_color='green',\n",
        "                                  keypoint_edge_width=2):\n",
        "  \"\"\"Draws keypoints on an image (numpy array).\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array with shape [height, width, 3].\n",
        "    keypoints: a numpy array with shape [num_keypoints, 2].\n",
        "    keypoint_scores: a numpy array with shape [num_keypoints]. If provided, only\n",
        "      those keypoints with a score above score_threshold will be visualized.\n",
        "    min_score_thresh: A scalar indicating the minimum keypoint score required\n",
        "      for a keypoint to be visualized. Note that keypoint_scores must be\n",
        "      provided for this threshold to take effect.\n",
        "    color: color to draw the keypoints with. Default is red.\n",
        "    radius: keypoint radius. Default value is 2.\n",
        "    use_normalized_coordinates: if True (default), treat keypoint values as\n",
        "      relative to the image.  Otherwise treat them as absolute.\n",
        "    keypoint_edges: A list of tuples with keypoint indices that specify which\n",
        "      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws\n",
        "      edges from keypoint 0 to 1 and from keypoint 2 to 4.\n",
        "    keypoint_edge_color: color to draw the keypoint edges with. Default is red.\n",
        "    keypoint_edge_width: width of the edges drawn between keypoints. Default\n",
        "      value is 2.\n",
        "  \"\"\"\n",
        "  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')\n",
        "  draw_keypoints_on_image(image_pil,\n",
        "                          keypoints,\n",
        "                          keypoint_scores=keypoint_scores,\n",
        "                          min_score_thresh=min_score_thresh,\n",
        "                          color=color,\n",
        "                          radius=radius,\n",
        "                          use_normalized_coordinates=use_normalized_coordinates,\n",
        "                          keypoint_edges=keypoint_edges,\n",
        "                          keypoint_edge_color=keypoint_edge_color,\n",
        "                          keypoint_edge_width=keypoint_edge_width)\n",
        "  np.copyto(image, np.array(image_pil))\n",
        "\n",
        "\n",
        "def draw_keypoints_on_image(image,\n",
        "                            keypoints,\n",
        "                            keypoint_scores=None,\n",
        "                            min_score_thresh=0.5,\n",
        "                            color='red',\n",
        "                            radius=2,\n",
        "                            use_normalized_coordinates=True,\n",
        "                            keypoint_edges=None,\n",
        "                            keypoint_edge_color='green',\n",
        "                            keypoint_edge_width=2):\n",
        "  \"\"\"Draws keypoints on an image.\n",
        "\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    keypoints: a numpy array with shape [num_keypoints, 2].\n",
        "    keypoint_scores: a numpy array with shape [num_keypoints].\n",
        "    min_score_thresh: a score threshold for visualizing keypoints. Only used if\n",
        "      keypoint_scores is provided.\n",
        "    color: color to draw the keypoints with. Default is red.\n",
        "    radius: keypoint radius. Default value is 2.\n",
        "    use_normalized_coordinates: if True (default), treat keypoint values as\n",
        "      relative to the image.  Otherwise treat them as absolute.\n",
        "    keypoint_edges: A list of tuples with keypoint indices that specify which\n",
        "      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws\n",
        "      edges from keypoint 0 to 1 and from keypoint 2 to 4.\n",
        "    keypoint_edge_color: color to draw the keypoint edges with. Default is red.\n",
        "    keypoint_edge_width: width of the edges drawn between keypoints. Default\n",
        "      value is 2.\n",
        "  \"\"\"\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  keypoints = np.array(keypoints)\n",
        "  keypoints_x = [k[1] for k in keypoints]\n",
        "  keypoints_y = [k[0] for k in keypoints]\n",
        "  if use_normalized_coordinates:\n",
        "    keypoints_x = tuple([im_width * x for x in keypoints_x])\n",
        "    keypoints_y = tuple([im_height * y for y in keypoints_y])\n",
        "  if keypoint_scores is not None:\n",
        "    keypoint_scores = np.array(keypoint_scores)\n",
        "    valid_kpt = np.greater(keypoint_scores, min_score_thresh)\n",
        "  else:\n",
        "    valid_kpt = np.where(np.any(np.isnan(keypoints), axis=1),\n",
        "                         np.zeros_like(keypoints[:, 0]),\n",
        "                         np.ones_like(keypoints[:, 0]))\n",
        "  valid_kpt = [v for v in valid_kpt]\n",
        "\n",
        "  for keypoint_x, keypoint_y, valid in zip(keypoints_x, keypoints_y, valid_kpt):\n",
        "    if valid:\n",
        "      draw.ellipse([(keypoint_x - radius, keypoint_y - radius),\n",
        "                    (keypoint_x + radius, keypoint_y + radius)],\n",
        "                   outline=color, fill=color)\n",
        "  if keypoint_edges is not None:\n",
        "    for keypoint_start, keypoint_end in keypoint_edges:\n",
        "      if (keypoint_start < 0 or keypoint_start >= len(keypoints) or\n",
        "          keypoint_end < 0 or keypoint_end >= len(keypoints)):\n",
        "        continue\n",
        "      if not (valid_kpt[keypoint_start] and valid_kpt[keypoint_end]):\n",
        "        continue\n",
        "      edge_coordinates = [\n",
        "          keypoints_x[keypoint_start], keypoints_y[keypoint_start],\n",
        "          keypoints_x[keypoint_end], keypoints_y[keypoint_end]\n",
        "      ]\n",
        "      draw.line(\n",
        "          edge_coordinates, fill=keypoint_edge_color, width=keypoint_edge_width)\n",
        "\n",
        "\n",
        "def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):\n",
        "  \"\"\"Draws mask on an image.\n",
        "\n",
        "  Args:\n",
        "    image: uint8 numpy array with shape (img_height, img_height, 3)\n",
        "    mask: a uint8 numpy array of shape (img_height, img_height) with\n",
        "      values between either 0 or 1.\n",
        "    color: color to draw the keypoints with. Default is red.\n",
        "    alpha: transparency value between 0 and 1. (default: 0.4)\n",
        "\n",
        "  Raises:\n",
        "    ValueError: On incorrect data type for image or masks.\n",
        "  \"\"\"\n",
        "  if image.dtype != np.uint8:\n",
        "    raise ValueError('`image` not of type np.uint8')\n",
        "  if mask.dtype != np.uint8:\n",
        "    raise ValueError('`mask` not of type np.uint8')\n",
        "  if image.shape[:2] != mask.shape:\n",
        "    raise ValueError('The image has spatial dimensions %s but the mask has '\n",
        "                     'dimensions %s' % (image.shape[:2], mask.shape))\n",
        "  rgb = ImageColor.getrgb(color)\n",
        "  pil_image = Image.fromarray(image)\n",
        "\n",
        "  solid_color = np.expand_dims(\n",
        "      np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n",
        "  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert('RGBA')\n",
        "  pil_mask = Image.fromarray(np.uint8(255.0*alpha*(mask > 0))).convert('L')\n",
        "  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
        "  np.copyto(image, np.array(pil_image.convert('RGB')))\n",
        "\n",
        "\n",
        "def draw_part_mask_on_image_array(image, mask, alpha=0.4, num_parts=24):\n",
        "  \"\"\"Draws part mask on an image.\n",
        "\n",
        "  Args:\n",
        "    image: uint8 numpy array with shape (img_height, img_height, 3)\n",
        "    mask: a uint8 numpy array of shape (img_height, img_height) with\n",
        "      1-indexed parts (0 for background).\n",
        "    alpha: transparency value between 0 and 1 (default: 0.4)\n",
        "    num_parts: the maximum number of parts that may exist in the image (default\n",
        "      24 for DensePose).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: On incorrect data type for image or masks.\n",
        "  \"\"\"\n",
        "  if image.dtype != np.uint8:\n",
        "    raise ValueError('`image` not of type np.uint8')\n",
        "  if mask.dtype != np.uint8:\n",
        "    raise ValueError('`mask` not of type np.uint8')\n",
        "  if image.shape[:2] != mask.shape:\n",
        "    raise ValueError('The image has spatial dimensions %s but the mask has '\n",
        "                     'dimensions %s' % (image.shape[:2], mask.shape))\n",
        "\n",
        "  pil_image = Image.fromarray(image)\n",
        "  part_colors = np.zeros_like(image)\n",
        "  mask_1_channel = mask[:, :, np.newaxis]\n",
        "  for i, color in enumerate(STANDARD_COLORS[:num_parts]):\n",
        "    rgb = np.array(ImageColor.getrgb(color), dtype=np.uint8)\n",
        "    part_colors += (mask_1_channel == i + 1) * rgb[np.newaxis, np.newaxis, :]\n",
        "  pil_part_colors = Image.fromarray(np.uint8(part_colors)).convert('RGBA')\n",
        "  pil_mask = Image.fromarray(np.uint8(255.0 * alpha * (mask > 0))).convert('L')\n",
        "  pil_image = Image.composite(pil_part_colors, pil_image, pil_mask)\n",
        "  np.copyto(image, np.array(pil_image.convert('RGB')))\n",
        "\n",
        "\n",
        "def draw_float_channel_on_image_array(image, channel, mask, alpha=0.9,\n",
        "                                      cmap='YlGn'):\n",
        "  \"\"\"Draws a floating point channel on an image array.\n",
        "\n",
        "  Args:\n",
        "    image: uint8 numpy array with shape (img_height, img_height, 3)\n",
        "    channel: float32 numpy array with shape (img_height, img_height). The values\n",
        "      should be in the range [0, 1], and will be mapped to colors using the\n",
        "      provided colormap `cmap` argument.\n",
        "    mask: a uint8 numpy array of shape (img_height, img_height) with\n",
        "      1-indexed parts (0 for background).\n",
        "    alpha: transparency value between 0 and 1 (default: 0.9)\n",
        "    cmap: string with the colormap to use.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: On incorrect data type for image or masks.\n",
        "  \"\"\"\n",
        "  if image.dtype != np.uint8:\n",
        "    raise ValueError('`image` not of type np.uint8')\n",
        "  if channel.dtype != np.float32:\n",
        "    raise ValueError('`channel` not of type np.float32')\n",
        "  if mask.dtype != np.uint8:\n",
        "    raise ValueError('`mask` not of type np.uint8')\n",
        "  if image.shape[:2] != channel.shape:\n",
        "    raise ValueError('The image has spatial dimensions %s but the channel has '\n",
        "                     'dimensions %s' % (image.shape[:2], channel.shape))\n",
        "  if image.shape[:2] != mask.shape:\n",
        "    raise ValueError('The image has spatial dimensions %s but the mask has '\n",
        "                     'dimensions %s' % (image.shape[:2], mask.shape))\n",
        "\n",
        "  cm = plt.get_cmap(cmap)\n",
        "  pil_image = Image.fromarray(image)\n",
        "  colored_channel = cm(channel)[:, :, :3]\n",
        "  pil_colored_channel = Image.fromarray(\n",
        "      np.uint8(colored_channel * 255)).convert('RGBA')\n",
        "  pil_mask = Image.fromarray(np.uint8(255.0 * alpha * (mask > 0))).convert('L')\n",
        "  pil_image = Image.composite(pil_colored_channel, pil_image, pil_mask)\n",
        "  np.copyto(image, np.array(pil_image.convert('RGB')))\n",
        "\n",
        "\n",
        "def visualize_boxes_and_labels_on_image_array(\n",
        "  image,\n",
        "  boxes,\n",
        "  classes,\n",
        "  scores,\n",
        "  category_index,\n",
        "  instance_masks=None,\n",
        "  instance_boundaries=None,\n",
        "  keypoints=None,\n",
        "  keypoint_scores=None,\n",
        "  keypoint_edges=None,\n",
        "  track_ids=None,\n",
        "  use_normalized_coordinates=False,\n",
        "  max_boxes_to_draw=20,\n",
        "  min_score_thresh=.5,\n",
        "  agnostic_mode=False,\n",
        "  line_thickness=4,\n",
        "  mask_alpha=.4,\n",
        "  groundtruth_box_visualization_color='black',\n",
        "  skip_boxes=False,\n",
        "  skip_scores=False,\n",
        "  skip_labels=False,\n",
        "  skip_track_ids=False):\n",
        "  box_to_display_str_map = collections.defaultdict(list)\n",
        "  box_to_color_map = collections.defaultdict(str)\n",
        "  box_to_instance_masks_map = {}\n",
        "  box_to_instance_boundaries_map = {}\n",
        "  box_to_keypoints_map = collections.defaultdict(list)\n",
        "  box_to_keypoint_scores_map = collections.defaultdict(list)\n",
        "  box_to_track_ids_map = {}\n",
        "  if not max_boxes_to_draw:\n",
        "    max_boxes_to_draw = boxes.shape[0]\n",
        "  for i in range(boxes.shape[0]):\n",
        "    if max_boxes_to_draw == len(box_to_color_map):\n",
        "      break\n",
        "    if scores is None or scores[i] > min_score_thresh:\n",
        "      box = tuple(boxes[i].tolist())\n",
        "      if instance_masks is not None:\n",
        "        box_to_instance_masks_map[box] = instance_masks[i]\n",
        "      if instance_boundaries is not None:\n",
        "        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n",
        "      if keypoints is not None:\n",
        "        box_to_keypoints_map[box].extend(keypoints[i])\n",
        "      if keypoint_scores is not None:\n",
        "        box_to_keypoint_scores_map[box].extend(keypoint_scores[i])\n",
        "      if track_ids is not None:\n",
        "        box_to_track_ids_map[box] = track_ids[i]\n",
        "      if scores is None:\n",
        "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
        "      else:\n",
        "        display_str = ''\n",
        "        if not skip_labels:\n",
        "          if not agnostic_mode:\n",
        "            if classes[i] in six.viewkeys(category_index):\n",
        "              class_name = category_index[classes[i]]['name']\n",
        "            else:\n",
        "              class_name = 'N/A'\n",
        "            display_str = str(class_name)\n",
        "            \n",
        "        final_label = display_str\n",
        "        if not skip_scores:\n",
        "          if not display_str:\n",
        "            display_str = '{}%'.format(round(100*scores[i]))\n",
        "            final_label = display_str\n",
        "          else:\n",
        "            display_str = '{}: {}%'.format(display_str, round(100*scores[i]))\n",
        "        if not skip_track_ids and track_ids is not None:\n",
        "          if not display_str:\n",
        "            display_str = 'ID {}'.format(track_ids[i])\n",
        "            final_label = track_ids[i]\n",
        "          else:\n",
        "            display_str = '{}: ID {}'.format(display_str, track_ids[i])\n",
        "        \n",
        "        box_to_display_str_map[box].append(display_str)\n",
        "        if agnostic_mode:\n",
        "          box_to_color_map[box] = 'DarkOrange'\n",
        "        elif track_ids is not None:\n",
        "          prime_multipler = _get_multiplier_for_color_randomness()\n",
        "          box_to_color_map[box] = STANDARD_COLORS[\n",
        "              (prime_multipler * track_ids[i]) % len(STANDARD_COLORS)]\n",
        "        else:\n",
        "          box_to_color_map[box] = STANDARD_COLORS[\n",
        "              classes[i] % len(STANDARD_COLORS)]\n",
        "\n",
        "  # Draw all boxes onto image.\n",
        "  for box, color in box_to_color_map.items():\n",
        "    ymin, xmin, ymax, xmax = box\n",
        "    if instance_masks is not None:\n",
        "      draw_mask_on_image_array(\n",
        "          image,\n",
        "          box_to_instance_masks_map[box],\n",
        "          color=color,\n",
        "          alpha=mask_alpha\n",
        "      )\n",
        "    if instance_boundaries is not None:\n",
        "      draw_mask_on_image_array(\n",
        "          image,\n",
        "          box_to_instance_boundaries_map[box],\n",
        "          color='red',\n",
        "          alpha=1.0\n",
        "      )\n",
        "    draw_bounding_box_on_image_array(\n",
        "        image,\n",
        "        ymin,\n",
        "        xmin,\n",
        "        ymax,\n",
        "        xmax,\n",
        "        color=color,\n",
        "        thickness=0 if skip_boxes else line_thickness,\n",
        "        display_str_list=box_to_display_str_map[box],\n",
        "        use_normalized_coordinates=use_normalized_coordinates)\n",
        "    if keypoints is not None:\n",
        "      keypoint_scores_for_box = None\n",
        "      if box_to_keypoint_scores_map:\n",
        "        keypoint_scores_for_box = box_to_keypoint_scores_map[box]\n",
        "      draw_keypoints_on_image_array(\n",
        "          image,\n",
        "          box_to_keypoints_map[box],\n",
        "          keypoint_scores_for_box,\n",
        "          min_score_thresh=min_score_thresh,\n",
        "          color=color,\n",
        "          radius=line_thickness / 2,\n",
        "          use_normalized_coordinates=use_normalized_coordinates,\n",
        "          keypoint_edges=keypoint_edges,\n",
        "          keypoint_edge_color=color,\n",
        "          keypoint_edge_width=line_thickness // 2)\n",
        "\n",
        "  return final_label, image\n",
        "\n",
        "\n",
        "def add_cdf_image_summary(values, name):\n",
        "  \"\"\"Adds a tf.summary.image for a CDF plot of the values.\n",
        "\n",
        "  Normalizes `values` such that they sum to 1, plots the cumulative distribution\n",
        "  function and creates a tf image summary.\n",
        "\n",
        "  Args:\n",
        "    values: a 1-D float32 tensor containing the values.\n",
        "    name: name for the image summary.\n",
        "  \"\"\"\n",
        "  def cdf_plot(values):\n",
        "    \"\"\"Numpy function to plot CDF.\"\"\"\n",
        "    normalized_values = values / np.sum(values)\n",
        "    sorted_values = np.sort(normalized_values)\n",
        "    cumulative_values = np.cumsum(sorted_values)\n",
        "    fraction_of_examples = (np.arange(cumulative_values.size, dtype=np.float32)\n",
        "                            / cumulative_values.size)\n",
        "    fig = plt.figure(frameon=False)\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.plot(fraction_of_examples, cumulative_values)\n",
        "    ax.set_ylabel('cumulative normalized values')\n",
        "    ax.set_xlabel('fraction of examples')\n",
        "    fig.canvas.draw()\n",
        "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
        "    image = np.fromstring(fig.canvas.tostring_rgb(), dtype='uint8').reshape(\n",
        "        1, int(height), int(width), 3)\n",
        "    return image\n",
        "  cdf_plot = tf.py_func(cdf_plot, [values], tf.uint8)\n",
        "  tf.summary.image(name, cdf_plot)\n",
        "\n",
        "\n",
        "def add_hist_image_summary(values, bins, name):\n",
        "  \"\"\"Adds a tf.summary.image for a histogram plot of the values.\n",
        "\n",
        "  Plots the histogram of values and creates a tf image summary.\n",
        "\n",
        "  Args:\n",
        "    values: a 1-D float32 tensor containing the values.\n",
        "    bins: bin edges which will be directly passed to np.histogram.\n",
        "    name: name for the image summary.\n",
        "  \"\"\"\n",
        "\n",
        "  def hist_plot(values, bins):\n",
        "    \"\"\"Numpy function to plot hist.\"\"\"\n",
        "    fig = plt.figure(frameon=False)\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    y, x = np.histogram(values, bins=bins)\n",
        "    ax.plot(x[:-1], y)\n",
        "    ax.set_ylabel('count')\n",
        "    ax.set_xlabel('value')\n",
        "    fig.canvas.draw()\n",
        "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
        "    image = np.fromstring(\n",
        "        fig.canvas.tostring_rgb(), dtype='uint8').reshape(\n",
        "            1, int(height), int(width), 3)\n",
        "    return image\n",
        "  hist_plot = tf.py_func(hist_plot, [values, bins], tf.uint8)\n",
        "  tf.summary.image(name, hist_plot)\n",
        "\n",
        "\n",
        "class EvalMetricOpsVisualization(six.with_metaclass(abc.ABCMeta, object)):\n",
        "  \"\"\"Abstract base class responsible for visualizations during evaluation.\n",
        "\n",
        "  Currently, summary images are not run during evaluation. One way to produce\n",
        "  evaluation images in Tensorboard is to provide tf.summary.image strings as\n",
        "  `value_ops` in tf.estimator.EstimatorSpec's `eval_metric_ops`. This class is\n",
        "  responsible for accruing images (with overlaid detections and groundtruth)\n",
        "  and returning a dictionary that can be passed to `eval_metric_ops`.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               category_index,\n",
        "               max_examples_to_draw=5,\n",
        "               max_boxes_to_draw=20,\n",
        "               min_score_thresh=0.2,\n",
        "               use_normalized_coordinates=True,\n",
        "               summary_name_prefix='evaluation_image',\n",
        "               keypoint_edges=None):\n",
        "    \"\"\"Creates an EvalMetricOpsVisualization.\n",
        "\n",
        "    Args:\n",
        "      category_index: A category index (dictionary) produced from a labelmap.\n",
        "      max_examples_to_draw: The maximum number of example summaries to produce.\n",
        "      max_boxes_to_draw: The maximum number of boxes to draw for detections.\n",
        "      min_score_thresh: The minimum score threshold for showing detections.\n",
        "      use_normalized_coordinates: Whether to assume boxes and keypoints are in\n",
        "        normalized coordinates (as opposed to absolute coordinates).\n",
        "        Default is True.\n",
        "      summary_name_prefix: A string prefix for each image summary.\n",
        "      keypoint_edges: A list of tuples with keypoint indices that specify which\n",
        "        keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws\n",
        "        edges from keypoint 0 to 1 and from keypoint 2 to 4.\n",
        "    \"\"\"\n",
        "\n",
        "    self._category_index = category_index\n",
        "    self._max_examples_to_draw = max_examples_to_draw\n",
        "    self._max_boxes_to_draw = max_boxes_to_draw\n",
        "    self._min_score_thresh = min_score_thresh\n",
        "    self._use_normalized_coordinates = use_normalized_coordinates\n",
        "    self._summary_name_prefix = summary_name_prefix\n",
        "    self._keypoint_edges = keypoint_edges\n",
        "    self._images = []\n",
        "\n",
        "  def clear(self):\n",
        "    self._images = []\n",
        "\n",
        "  def add_images(self, images):\n",
        "    \"\"\"Store a list of images, each with shape [1, H, W, C].\"\"\"\n",
        "    if len(self._images) >= self._max_examples_to_draw:\n",
        "      return\n",
        "\n",
        "    # Store images and clip list if necessary.\n",
        "    self._images.extend(images)\n",
        "    if len(self._images) > self._max_examples_to_draw:\n",
        "      self._images[self._max_examples_to_draw:] = []\n",
        "\n",
        "  def get_estimator_eval_metric_ops(self, eval_dict):\n",
        "    \"\"\"Returns metric ops for use in tf.estimator.EstimatorSpec.\n",
        "\n",
        "    Args:\n",
        "      eval_dict: A dictionary that holds an image, groundtruth, and detections\n",
        "        for a batched example. Note that, we use only the first example for\n",
        "        visualization. See eval_util.result_dict_for_batched_example() for a\n",
        "        convenient method for constructing such a dictionary. The dictionary\n",
        "        contains\n",
        "        fields.InputDataFields.original_image: [batch_size, H, W, 3] image.\n",
        "        fields.InputDataFields.original_image_spatial_shape: [batch_size, 2]\n",
        "          tensor containing the size of the original image.\n",
        "        fields.InputDataFields.true_image_shape: [batch_size, 3]\n",
        "          tensor containing the spatial size of the upadded original image.\n",
        "        fields.InputDataFields.groundtruth_boxes - [batch_size, num_boxes, 4]\n",
        "          float32 tensor with groundtruth boxes in range [0.0, 1.0].\n",
        "        fields.InputDataFields.groundtruth_classes - [batch_size, num_boxes]\n",
        "          int64 tensor with 1-indexed groundtruth classes.\n",
        "        fields.InputDataFields.groundtruth_instance_masks - (optional)\n",
        "          [batch_size, num_boxes, H, W] int64 tensor with instance masks.\n",
        "        fields.InputDataFields.groundtruth_keypoints - (optional)\n",
        "          [batch_size, num_boxes, num_keypoints, 2] float32 tensor with\n",
        "          keypoint coordinates in format [y, x].\n",
        "        fields.InputDataFields.groundtruth_keypoint_visibilities - (optional)\n",
        "          [batch_size, num_boxes, num_keypoints] bool tensor with\n",
        "          keypoint visibilities.\n",
        "        fields.DetectionResultFields.detection_boxes - [batch_size,\n",
        "          max_num_boxes, 4] float32 tensor with detection boxes in range [0.0,\n",
        "          1.0].\n",
        "        fields.DetectionResultFields.detection_classes - [batch_size,\n",
        "          max_num_boxes] int64 tensor with 1-indexed detection classes.\n",
        "        fields.DetectionResultFields.detection_scores - [batch_size,\n",
        "          max_num_boxes] float32 tensor with detection scores.\n",
        "        fields.DetectionResultFields.detection_masks - (optional) [batch_size,\n",
        "          max_num_boxes, H, W] float32 tensor of binarized masks.\n",
        "        fields.DetectionResultFields.detection_keypoints - (optional)\n",
        "          [batch_size, max_num_boxes, num_keypoints, 2] float32 tensor with\n",
        "          keypoints.\n",
        "        fields.DetectionResultFields.detection_keypoint_scores - (optional)\n",
        "          [batch_size, max_num_boxes, num_keypoints] float32 tensor with\n",
        "          keypoints scores.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary of image summary names to tuple of (value_op, update_op). The\n",
        "      `update_op` is the same for all items in the dictionary, and is\n",
        "      responsible for saving a single side-by-side image with detections and\n",
        "      groundtruth. Each `value_op` holds the tf.summary.image string for a given\n",
        "      image.\n",
        "    \"\"\"\n",
        "    if self._max_examples_to_draw == 0:\n",
        "      return {}\n",
        "    images = self.images_from_evaluation_dict(eval_dict)\n",
        "\n",
        "    def get_images():\n",
        "      \"\"\"Returns a list of images, padded to self._max_images_to_draw.\"\"\"\n",
        "      images = self._images\n",
        "      while len(images) < self._max_examples_to_draw:\n",
        "        images.append(np.array(0, dtype=np.uint8))\n",
        "      self.clear()\n",
        "      return images\n",
        "\n",
        "    def image_summary_or_default_string(summary_name, image):\n",
        "      \"\"\"Returns image summaries for non-padded elements.\"\"\"\n",
        "      return tf.cond(\n",
        "          tf.equal(tf.size(tf.shape(image)), 4),\n",
        "          lambda: tf.summary.image(summary_name, image),\n",
        "          lambda: tf.constant(''))\n",
        "\n",
        "    if tf.executing_eagerly():\n",
        "      update_op = self.add_images([[images[0]]])\n",
        "      image_tensors = get_images()\n",
        "    else:\n",
        "      update_op = tf.py_func(self.add_images, [[images[0]]], [])\n",
        "      image_tensors = tf.py_func(\n",
        "          get_images, [], [tf.uint8] * self._max_examples_to_draw)\n",
        "    eval_metric_ops = {}\n",
        "    for i, image in enumerate(image_tensors):\n",
        "      summary_name = self._summary_name_prefix + '/' + str(i)\n",
        "      value_op = image_summary_or_default_string(summary_name, image)\n",
        "      eval_metric_ops[summary_name] = (value_op, update_op)\n",
        "    return eval_metric_ops\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def images_from_evaluation_dict(self, eval_dict):\n",
        "    \"\"\"Converts evaluation dictionary into a list of image tensors.\n",
        "\n",
        "    To be overridden by implementations.\n",
        "\n",
        "    Args:\n",
        "      eval_dict: A dictionary with all the necessary information for producing\n",
        "        visualizations.\n",
        "\n",
        "    Returns:\n",
        "      A list of [1, H, W, C] uint8 tensors.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "class VisualizeSingleFrameDetections(EvalMetricOpsVisualization):\n",
        "  \"\"\"Class responsible for single-frame object detection visualizations.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               category_index,\n",
        "               max_examples_to_draw=5,\n",
        "               max_boxes_to_draw=20,\n",
        "               min_score_thresh=0.2,\n",
        "               use_normalized_coordinates=True,\n",
        "               summary_name_prefix='Detections_Left_Groundtruth_Right',\n",
        "               keypoint_edges=None):\n",
        "    super(VisualizeSingleFrameDetections, self).__init__(\n",
        "        category_index=category_index,\n",
        "        max_examples_to_draw=max_examples_to_draw,\n",
        "        max_boxes_to_draw=max_boxes_to_draw,\n",
        "        min_score_thresh=min_score_thresh,\n",
        "        use_normalized_coordinates=use_normalized_coordinates,\n",
        "        summary_name_prefix=summary_name_prefix,\n",
        "        keypoint_edges=keypoint_edges)\n",
        "\n",
        "  def images_from_evaluation_dict(self, eval_dict):\n",
        "    return draw_side_by_side_evaluation_image(eval_dict, self._category_index,\n",
        "                                              self._max_boxes_to_draw,\n",
        "                                              self._min_score_thresh,\n",
        "                                              self._use_normalized_coordinates,\n",
        "                                              self._keypoint_edges)"
      ],
      "metadata": {
        "id": "R7Dd_q37wB0Q"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Xp9aYC-AG5Mk"
      },
      "outputs": [],
      "source": [
        "import object_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "_NHwMAItPmq7",
        "outputId": "096551c9-72cb-43dd-cd54-2bf19829fd8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...Done! Took 8.179731845855713 seconds\n",
            "Running inference for /content/drive/MyDrive/training_demo/images/denemelıkler/indir (1).jpg... Done\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=300x168 at 0x7F4A49D88DD0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "closed\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Object Detection (On Image) From TF2 Saved Model\n",
        "=====================================\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import argparse\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Enable GPU dynamic memory allocation\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# PROVIDE PATH TO IMAGE DIRECTORY\n",
        "IMAGE_PATHS = '/content/drive/MyDrive/training_demo/images/denemelıkler/indir (1).jpg'\n",
        "\n",
        "\n",
        "# PROVIDE PATH TO MODEL DIRECTORY\n",
        "PATH_TO_MODEL_DIR = '/content/drive/MyDrive/training_demo/exported-models/my_model'\n",
        "\n",
        "# PROVIDE PATH TO LABEL MAP\n",
        "PATH_TO_LABELS = '/content/drive/MyDrive/training_demo/annotations/label_map.pbtxt'\n",
        "\n",
        "# PROVIDE THE MINIMUM CONFIDENCE THRESHOLD\n",
        "MIN_CONF_THRESH = float(0.60)\n",
        "\n",
        "# LOAD THE MODEL\n",
        "\n",
        "import time\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils\n",
        "import collections\n",
        "\n",
        "\n",
        "\n",
        "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
        "\n",
        "print('Loading model...', end='')\n",
        "start_time = time.time()\n",
        "\n",
        "# LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
        "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))\n",
        "\n",
        "# LOAD LABEL MAP DATA FOR PLOTTING\n",
        "\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
        "                                                                    use_display_name=True)\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Running inference for {}... '.format(IMAGE_PATHS), end='')\n",
        "\n",
        "image = cv2.imread(IMAGE_PATHS)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image_expanded = np.expand_dims(image_rgb, axis=0)\n",
        "\n",
        "# The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "input_tensor = tf.convert_to_tensor(image)\n",
        "# The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "# input_tensor = np.expand_dims(image_np, 0)\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "# All outputs are batches tensors.\n",
        "# Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "# We're only interested in the first num_detections.\n",
        "num_detections = int(detections.pop('num_detections'))\n",
        "detections = {key: value[0, :num_detections].numpy()\n",
        "               for key, value in detections.items()}\n",
        "detections['num_detections'] = num_detections\n",
        "\n",
        "# detection_classes should be ints.\n",
        "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "image_with_detections = image.copy()\n",
        "\n",
        "# SET MIN_SCORE_THRESH BASED ON YOU MINIMUM THRESHOLD FOR DETECTIONS\n",
        "label, _ = visualize_boxes_and_labels_on_image_array(\n",
        "      image_with_detections,\n",
        "      detections['detection_boxes'],\n",
        "      detections['detection_classes'],\n",
        "      detections['detection_scores'],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=80,\n",
        "      min_score_thresh=0.3,\n",
        "      agnostic_mode=False)\n",
        "\n",
        "print('Done')\n",
        "# DISPLAYS OUTPUT IMAGE\n",
        "cv2_imshow(image_with_detections)\n",
        "print(label)\n",
        "# CLOSES WINDOW ONCE KEY IS PRESSED\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKvBbICWPmxz",
        "outputId": "c777078f-b92a-4b6c-9cad-2e65a173934b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.9.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n",
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0521 07:54:01.525769 140319609395072 model_lib_v2.py:1090] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\n",
            "I0521 07:54:01.525998 140319609395072 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0521 07:54:01.645140 140319609395072 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0521 07:54:01.645274 140319609395072 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0521 07:54:01.645454 140319609395072 model_lib_v2.py:1110] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/drive/MyDrive/training_demo/annotations/test.record']\n",
            "I0521 07:54:02.543617 140319609395072 dataset_builder.py:162] Reading unweighted datasets: ['/content/drive/MyDrive/training_demo/annotations/test.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/drive/MyDrive/training_demo/annotations/test.record']\n",
            "I0521 07:54:02.544114 140319609395072 dataset_builder.py:79] Reading record datasets for input file: ['/content/drive/MyDrive/training_demo/annotations/test.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0521 07:54:02.544269 140319609395072 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0521 07:54:02.544343 140319609395072 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W0521 07:54:02.548597 140319609395072 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0521 07:54:02.587373 140319609395072 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0521 07:54:06.348275 140319609395072 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0521 07:54:07.679133 140319609395072 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Waiting for new checkpoint at /content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50\n",
            "I0521 07:54:10.171669 140319609395072 checkpoint_utils.py:136] Waiting for new checkpoint at /content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50\n",
            "INFO:tensorflow:Found new checkpoint at /content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50/ckpt-11\n",
            "I0521 07:54:10.558759 140319609395072 checkpoint_utils.py:145] Found new checkpoint at /content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50/ckpt-11\n",
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0521 07:54:18.816205 140319609395072 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "W0521 07:54:25.445968 140319609395072 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0521 07:54:32.851812 140319609395072 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0521 07:54:48.259986 140319609395072 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Finished eval step 0\n",
            "I0521 07:54:48.269000 140319609395072 model_lib_v2.py:966] Finished eval step 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:459: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0521 07:54:48.389645 140319609395072 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:459: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "INFO:tensorflow:Performing evaluation on 22 images.\n",
            "I0521 07:54:53.445540 140319609395072 coco_evaluation.py:293] Performing evaluation on 22 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0521 07:54:53.445905 140319609395072 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0521 07:54:53.448252 140319609395072 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.14s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.03s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.429\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.522\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.184\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.578\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.262\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.557\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.578\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.394\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.651\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
            "INFO:tensorflow:Eval metrics at step 10000\n",
            "I0521 07:54:53.627575 140319609395072 model_lib_v2.py:1015] Eval metrics at step 10000\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP: 0.429050\n",
            "I0521 07:54:53.633064 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP: 0.429050\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP@.50IOU: 0.688100\n",
            "I0521 07:54:53.634542 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP@.50IOU: 0.688100\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP@.75IOU: 0.521672\n",
            "I0521 07:54:53.635921 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP@.75IOU: 0.521672\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (small): 0.183538\n",
            "I0521 07:54:53.637181 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP (small): 0.183538\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (medium): 0.577841\n",
            "I0521 07:54:53.638456 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP (medium): 0.577841\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Precision/mAP (large): -1.000000\n",
            "I0521 07:54:53.639726 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Precision/mAP (large): -1.000000\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@1: 0.261765\n",
            "I0521 07:54:53.640979 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@1: 0.261765\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@10: 0.557475\n",
            "I0521 07:54:53.642179 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@10: 0.557475\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100: 0.578064\n",
            "I0521 07:54:53.643390 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@100: 0.578064\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (small): 0.394048\n",
            "I0521 07:54:53.644653 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@100 (small): 0.394048\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (medium): 0.651111\n",
            "I0521 07:54:53.645952 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@100 (medium): 0.651111\n",
            "INFO:tensorflow:\t+ DetectionBoxes_Recall/AR@100 (large): -1.000000\n",
            "I0521 07:54:53.647169 140319609395072 model_lib_v2.py:1018] \t+ DetectionBoxes_Recall/AR@100 (large): -1.000000\n",
            "INFO:tensorflow:\t+ Loss/RPNLoss/localization_loss: 0.022915\n",
            "I0521 07:54:53.648134 140319609395072 model_lib_v2.py:1018] \t+ Loss/RPNLoss/localization_loss: 0.022915\n",
            "INFO:tensorflow:\t+ Loss/RPNLoss/objectness_loss: 0.018591\n",
            "I0521 07:54:53.649128 140319609395072 model_lib_v2.py:1018] \t+ Loss/RPNLoss/objectness_loss: 0.018591\n",
            "INFO:tensorflow:\t+ Loss/BoxClassifierLoss/localization_loss: 0.026205\n",
            "I0521 07:54:53.650099 140319609395072 model_lib_v2.py:1018] \t+ Loss/BoxClassifierLoss/localization_loss: 0.026205\n",
            "INFO:tensorflow:\t+ Loss/BoxClassifierLoss/classification_loss: 0.130483\n",
            "I0521 07:54:53.651062 140319609395072 model_lib_v2.py:1018] \t+ Loss/BoxClassifierLoss/classification_loss: 0.130483\n",
            "INFO:tensorflow:\t+ Loss/regularization_loss: 0.000000\n",
            "I0521 07:54:53.652035 140319609395072 model_lib_v2.py:1018] \t+ Loss/regularization_loss: 0.000000\n",
            "INFO:tensorflow:\t+ Loss/total_loss: 0.198194\n",
            "I0521 07:54:53.653002 140319609395072 model_lib_v2.py:1018] \t+ Loss/total_loss: 0.198194\n",
            "INFO:tensorflow:Exiting evaluation at step 10000\n",
            "I0521 07:54:53.806681 140319609395072 model_lib_v2.py:1168] Exiting evaluation at step 10000\n"
          ]
        }
      ],
      "source": [
        " !python model_main_tf2.py --model_dir= /content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50 --pipeline_config_path=/content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50/pipeline.config --checkpoint_dir=/content/drive/MyDrive/training_demo/models/faster_rcnn_restnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KYzKvUCw6tXH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tez7uZeaPm0J",
        "outputId": "c30cc82f-9d06-4d1b-c045-a6900e4f9f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/training_demo/eval\n"
          ]
        }
      ],
      "source": [
        "cd eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "B1ac3Aax6W4k"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vg2kHCsqOxlU"
      },
      "outputs": [],
      "source": [
        "#  %tensorboard --logdir=.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euKHVVE_7KKs",
        "outputId": "18ea8ae3-431d-4bc3-b72e-69484d571fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/training_demo\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/training_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9J0qFz5zOxnM",
        "outputId": "204ee8b5-adae-4f32-aeda-ee113180ccd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-3.17.0-py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 34.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.9.0)\n",
            "Collecting packaging~=20.9\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging~=20.9->tensorflowjs) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (4.2.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (57.4.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.46.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.20.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.21.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (2.27.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<3,>=2.1.0->tensorflowjs) (3.2.0)\n",
            "Installing collected packages: packaging, tensorflowjs\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed packaging-20.9 tensorflowjs-3.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A8lc93yITpQ",
        "outputId": "01947104-8082-4bb9-e937-981ea65375ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing weight file /content/drive/MyDrive/training_demo/models/ssd_mobilenet_v2_fpn_keras/tfjsexport/model.json...\n"
          ]
        }
      ],
      "source": [
        "!tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes,detection_classes,detection_features,detection_multiclass_scores,detection_scores,num_detections,raw_detection_boxes,raw_detection_scores' --output_format=tfjs_graph_model --signature_name=serving_default /content/drive/MyDrive/training_demo/exported-models/my_model/saved_model /content/drive/MyDrive/training_demo/models/ssd_mobilenet_v2_fpn_keras/tfjsexport"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}